{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Vme9lIN6MvBXjExXyIXp_srl1CYReNuG","timestamp":1710106713691}],"authorship_tag":"ABX9TyMjUQifHRndYzq7dvuSRpmb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["###Overview\n"," This file is a variation of Mamba-tiny by https://github.com/PeaBrane/mamba-tiny/tree/master\n","\n"," It contains a compact version of Mamba which is fully functional, but does not implement the efficient GPU parts use or parallel scan.\n","\n","I addition to the changes done in 'Mamba model', this model has the additional functionality of running sequentially, so that in inference the new datapoints ar entered one by one. This means that the hidden states are handled such that they can be saved from step to step and not start automatically from zeros.\\\n","To work in this mode, the model needs to be intiated with 'generate = True' in the model args configuration.\\\n","If we wish to reset the model to start a new sequence, we can execute the 'reset_h' method.\n","\n","After constructing this model variation we added the 'generate_k_last_predictions' function and made sure that we get the same results for runnig it with k=1 as for the non generative model operation."],"metadata":{"id":"4VzsGYNE3n1S"}},{"cell_type":"code","source":["import io\n","import os\n","import sys\n","import copy\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDZ8zvK6wHZV","executionInfo":{"status":"ok","timestamp":1715524144383,"user_tz":-180,"elapsed":2462,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"c9ef2a8f-8b2b-4052-f690-50dce028afdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/Final Project UAV/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FJvriUDy2sJ","executionInfo":{"status":"ok","timestamp":1715524152001,"user_tz":-180,"elapsed":287,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"6d803102-3af7-42e9-9a22-aed321c36de7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}]},{"cell_type":"code","source":["\"\"\"Simple, minimal implementation of Mamba in one file of PyTorch.\n","\n","Suggest reading the following before/while reading the code:\n","    [1] Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Albert Gu and Tri Dao)\n","        https://arxiv.org/abs/2312.00752\n","    [2] The Annotated S4 (Sasha Rush and Sidd Karamcheti)\n","        https://srush.github.io/annotated-s4\n","\n","Glossary:\n","    b: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n","    l: sequence length                  (`L` in [1] Algorithm 2)\n","    d or d_model: hidden dim\n","    n or d_state: latent state dim      (`N` in [1] Algorithm 2)\n","    expand: expansion factor            (`E` in [1] Section 3.4)\n","    d_in or d_inner: d * expand         (`D` in [1] Algorithm 2)\n","    A, B, C, D: state space parameters  (See any state space representation formula)\n","                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n","    Δ or delta: input-dependent step size\n","    dt_rank: rank of Δ                  (See [1] Section 3.6 \"Parameterization of ∆\")\n","\n","\"\"\"\n","!pip install einops\n","from __future__ import annotations\n","import math\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from dataclasses import dataclass\n","from einops import rearrange, repeat, einsum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPSBWF3RXztS","executionInfo":{"status":"ok","timestamp":1715524081786,"user_tz":-180,"elapsed":8857,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"dcfbbac8-3ce3-4715-e234-dedd666fa18d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}]},{"cell_type":"code","source":["@dataclass\n","class ModelArgs:\n","    d_model: int\n","    n_layer: int\n","    d_state: int = 16\n","    expand: int = 2\n","    d_conv: int = 4\n","    pad_vocab_size_multiple: int = 8\n","    conv_bias: bool = True\n","    bias: bool = False\n","    generate: bool = False\n","    h_revision: int = 0\n","\n","    def reset_h(self):\n","        self.h_revision += 1\n","\n","    def __post_init__(self):\n","        self.d_inner = int(self.expand * self.d_model)\n"],"metadata":{"id":"wWgcKZYFX8ZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Mamba(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        \"\"\"Full Mamba model.\"\"\"\n","        super().__init__()\n","        self.args = args\n","        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n","        self.norm_f = RMSNorm(args.d_model)\n","\n","    def forward(self, x, delta, h = None):\n","        \"\"\"\n","        Args:\n","            x (long tensor): shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n","            delta (float tensor): shape (b, l, 1) --> this is the added time interval vector\n","        Returns:\n","            output: shape (b, l, d) (prediction of future values)\n","\n","        Official Implementation:\n","            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n","\n","        \"\"\"\n","        for layer in self.layers:\n","            x = layer(x, delta)\n","\n","        output = self.norm_f(x)\n","\n","        return output"],"metadata":{"id":"CrO89I4qYCrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n","        super().__init__()\n","        self.args = args\n","        self.mixer = MambaBlock(args)\n","        self.norm = RMSNorm(args.d_model)\n","\n","\n","    def forward(self, x, delta):\n","        \"\"\"\n","        Args:\n","            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n","            delta: shape (b, l, 1) --> this is the added time interval vector\n","        Returns:\n","            output: shape (b, l, d)\n","\n","        Official Implementation:\n","            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n","\n","            Note: the official repo chains residual blocks that look like\n","                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n","            where the first Add is a no-op. This is purely for performance reasons as this\n","            allows them to fuse the Add->Norm.\n","\n","            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n","                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n","\n","        \"\"\"\n","        output = self.mixer(self.norm(x), delta) + x\n","\n","        return output"],"metadata":{"id":"2Pl1nPcRYINX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MambaBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n","        super().__init__()\n","        self.args = args\n","\n","        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n","\n","        self.conv1d = nn.Conv1d(\n","            in_channels=args.d_inner,\n","            out_channels=args.d_inner,\n","            bias=args.conv_bias,\n","            kernel_size=args.d_conv,\n","            groups=args.d_inner,\n","            padding=args.d_conv - 1,\n","        )\n","\n","        # x_proj takes in `x` and outputs the input-specific B, C\n","        self.x_proj = nn.Linear(args.d_inner, args.d_state * 2, bias=False)\n","\n","        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n","        self.A_log = nn.Parameter(torch.log(A))\n","        self.D = nn.Parameter(torch.ones(args.d_inner))\n","        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n","        self.h = None\n","        self.h_revision = args.h_revision\n","\n","    def forward(self, x, delta):\n","        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n","\n","        Args:\n","            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n","            delta: shape (b, l, 1)\n","        Returns:\n","            output: shape (b, l, d)\n","\n","        Official Implementation:\n","            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n","            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n","\n","        \"\"\"\n","        (b, l, d) = x.shape\n","\n","        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n","        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n","\n","        x = rearrange(x, 'b l d_in -> b d_in l')\n","        x = self.conv1d(x)[:, :, :l]\n","        x = rearrange(x, 'b d_in l -> b l d_in')\n","\n","        x = F.silu(x)\n","\n","        y = self.ssm(x, delta)\n","\n","        y = y * F.silu(res)\n","\n","        output = self.out_proj(y)\n","\n","        return output\n","\n","\n","    def ssm(self, x, delta):\n","        \"\"\"Runs the SSM. See:\n","            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n","            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n","\n","        Args:\n","            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n","            delta: shape (b, l, 1)\n","        Returns:\n","            output: shape (b, l, d_in)\n","\n","        Official Implementation:\n","            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n","\n","        \"\"\"\n","        (d_in, n) = self.A_log.shape\n","\n","        # Compute A B C D, the state space parameters.\n","        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n","        #     B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n","        #                                  and is why Mamba is called **selective** state spaces)\n","\n","        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n","        D = self.D.float()\n","        delta = torch.unsqueeze(delta, -1).repeat(1, 1, d_in)  # shape (b, l, d_in) --> using the same delta to all parallel inputs\n","\n","        x_bl = self.x_proj(x)  # (b, l, 2*n)\n","        (B, C) = x_bl.split(split_size=[n, n], dim=-1)  # B, C: (b, l, n)\n","\n","        y = self.selective_scan(x, delta, A, B, C, D)  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n","\n","        return y\n","\n","\n","    def selective_scan(self, x, delta, A, B, C, D):\n","        \"\"\"Does selective scan algorithm. See:\n","            - Section 2 State Space Models in the Mamba paper [1]\n","            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n","            - run_SSM(A, B, C, x) in The Annotated S4 [2]\n","\n","        This is the classic discrete state space formula:\n","            h(t + 1) = Ah(t) + Bx(t)\n","            y(t)     = Ch(t) + Dx(t)\n","        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n","\n","        Args:\n","            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n","            delta: shape (b, l, d_in)\n","            A: shape (d_in, n)\n","            B: shape (b, l, n)\n","            C: shape (b, l, n)\n","            D: shape (d_in,)\n","\n","        Returns:\n","            output: shape (b, l, d_in)\n","\n","        Official Implementation:\n","            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n","            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n","\n","        \"\"\"\n","        (b, l, d_in) = x.shape\n","        n = A.shape[1]\n","\n","        # Discretize continuous parameters (A, B)\n","        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n","        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n","        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n","        deltaA = torch.exp(einsum(delta, A, 'b l d_in, d_in n -> b l d_in n'))\n","        deltaB_x = einsum(delta, B, x, 'b l d_in, b l n, b l d_in -> b l d_in n')\n","\n","        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n","        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n","        # is additionally hardware-aware (like FlashAttention).\n","\n","        # Handling the hidden state with the user settings.\n","        # Either null h or load it from the last recorded self.h\n","        if self.args.h_revision > self.h_revision:\n","          self.h  = None\n","          self.h_revision = self.args.h_revision\n","          # resetting h\n","\n","        if self.h is None or not self.args.generate:\n","          h = torch.zeros((b, d_in, n), device=deltaA.device)\n","        else:\n","          h = self.h\n","\n","        ys = []\n","        for i in range(l):\n","            h = deltaA[:, i] * h + deltaB_x[:, i]\n","            y = einsum(h, C[:, i, :], 'b d_in n, b n -> b d_in')\n","            ys.append(y)\n","        self.h = h\n","        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n","        y = y + x * D\n","\n","        return y"],"metadata":{"id":"uuHgeDhYYPgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RMSNorm(nn.Module):\n","    def __init__(self,\n","                 d_model: int,\n","                 eps: float = 1e-5):\n","        super().__init__()\n","        self.eps = eps\n","        self.weight = nn.Parameter(torch.ones(d_model))\n","\n","\n","    def forward(self, x):\n","        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n","\n","        return output\n"],"metadata":{"id":"pOWp8bCWXskA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Testing the model on our samples"],"metadata":{"id":"DXeL4jKcijJg"}},{"cell_type":"code","source":["save_path = './Samples/mamba_samples_testing0_' + samples_config['subfolder'] + '_samples'\n","with open(save_path , 'rb') as f:\n","  train_samples, test_samples, train_samples_filenames, test_samples_filenames, train_dt, test_dt = pickle.load(f)"],"metadata":{"id":"29NGkI1SyFFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_data = list(zip(train_samples, train_dt))\n","dl_test0 = DataLoader(test_data, batch_size = 1)\n","sample, dt = next(iter(dl_test0))\n","sample.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bWAEtfyQ0J4U","executionInfo":{"status":"ok","timestamp":1715524181073,"user_tz":-180,"elapsed":2,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"e09d0257-e4f9-45cc-8839-51c6c2866d88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 251, 8])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["d_model = 8\n","n_layer = 1\n","d_state = 16\n","expand = 2\n","d_conv = 4\n","# conv_bias = True\n","# bias = False\n","generate = True\n","args = ModelArgs(d_model, n_layer, d_state, expand, d_conv, generate=generate)"],"metadata":{"id":"owCHXj-43tOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1 = Mamba(args)\n","model2 = copy.deepcopy(model1)"],"metadata":{"id":"N8LFJIxK4joV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = model1(sample, dt)"],"metadata":{"id":"DF4xRwdl49M3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output.shape"],"metadata":{"id":"rMQixk3aIpzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output[0,-10:, 0])"],"metadata":{"id":"_CHx_p_nIsCb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715524246158,"user_tz":-180,"elapsed":281,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"18de8c93-cf20-4e82-a5a4-bec1b10918b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-2.7939, -2.8003, -2.7975, -2.7481, -2.7613, -2.7999, -2.8008, -2.7924,\n","        -2.7449, -2.7335], grad_fn=<SelectBackward0>)\n"]}]},{"cell_type":"markdown","source":["###Generate"],"metadata":{"id":"-53cE5h8go9E"}},{"cell_type":"code","source":["def generate_k_last_predictions(model, k, sample, dt):\n","  sample_header = sample[:, :-k, :]\n","  dt_header = dt[:, :-k]\n","  dt_tail = dt[:, -k:]\n","\n","  with torch.no_grad():\n","    output = model(sample_header, dt_header)\n","    y = output[:, -1, :].unsqueeze(1) #generating from last prediction\n","    pred = [y]\n","    for i in range(k):\n","      single_dt = dt_tail[:, i].unsqueeze(1)\n","      y = model(y, single_dt)\n","      pred.append(y)\n","    predictions = torch.cat(pred, dim=1)\n","      # !note that the last prediction here does not have a label and should not be used. returning list of k+1 with only first k relevant\n","  return predictions"],"metadata":{"id":"5Gb12QbWgEuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = generate_k_last_predictions(model2, 5, sample, dt)"],"metadata":{"id":"Poh1e2zeAey7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions"],"metadata":{"id":"W2U3jlQ3VEGx"},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **Overview**\n","\n","This notebooks contains tools for preprocessing and visualization of the Walaris dataset for flight trajectories, given as Json files.\n"]},{"cell_type":"markdown","source":["### To Do\n"],"metadata":{"id":"AbMGQuI36K3c"}},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30122,"status":"ok","timestamp":1705958358357,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"},"user_tz":-120},"id":"q2hR_rZ4hcwY","outputId":"d77ce299-5760-4468-f271-dae20b637d24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","from sklearn.utils import class_weight\n","import xgboost as xgb\n","\n","\n","# prompt: Access a folder on google drive and import the data locally\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":807,"status":"ok","timestamp":1705958359160,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"},"user_tz":-120},"id":"EP0cLHeSozdF","outputId":"4d4cb022-8941-4d33-d03d-45534e379c24"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/bird/VIS_bird_20230605-406_20230605-142825.json\"\n","# file_path = \"track_data/bird/VIS_bird_20231024-22251_20231024-083633.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231023-6210_20231023-162937.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231025-21925_20231025-074948.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10903_20231027-114017.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-11132_20231027-115333.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10582_20231027-113033.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231030-6113_20231030-140702.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10437_20231027-112435.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# plot_vs_time(file_path, 'vel')"],"metadata":{"id":"TNVFI13GYV1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Segmentation"],"metadata":{"id":"EmuJtXUpZujJ"}},{"cell_type":"markdown","source":["Assume interpolation of dt = 40 msec.\\\n","For example: a sequence of 5 seconds is equivalent to 25*5 = 125"],"metadata":{"id":"RleKlQDgaQHZ"}},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","  sample_duration = samples_config['sample_duration']\n","  overlap_factor = samples_config['overlap_factor']\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","  complete_sample = np.stack([tt, xx, yy, zz, size_hor, size_ver]).T\n","\n","  #segmenting\n","  skip = sample_duration * (1-overlap_factor)\n","  current_index = [0]\n","  end_index = np.nonzero(tt>=tt[0]+sample_duration)[0]\n","\n","  sub_samples = []\n","  while end_index.size:\n","      # print(current_index[0], end_index[0])\n","      if end_index[0] - current_index[0] > samples_config['min_samples']: ### a threshold for a minimum number of datapoints in a sample\n","        sub_samples.append(complete_sample[current_index[0]:end_index[0], :])\n","      end_index = np.nonzero(tt>=tt[current_index[0]]+skip+sample_duration)[0]\n","      current_index = np.nonzero(tt>=tt[current_index[0]]+skip)[0]\n","\n","  return sub_samples, light_domain"],"metadata":{"id":"RL5aYuYSd0v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["subf_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KbmQvmQbP3U","executionInfo":{"status":"ok","timestamp":1705958359160,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"00994bdd-15f5-40b4-b6f8-88fcf4f4d992"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  subfolders = os.listdir(folder)\n","  subfolders_list = [subfolders[i] for i in samples_config['subfolders_ind']]\n","\n","  samples_dict = {}\n","  samples_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","      print(subfolder)\n","      total_samples = 0\n","      subfolder_path = os.path.join(folder, subfolder)\n","      files = os.listdir(subfolder_path)\n","      for file in files:\n","          file_path = os.path.join(subfolder_path, file)\n","          sub_samples, light_domain = segment_file_by_time(file_path, samples_config)\n","          samples_dict[file] = sub_samples\n","          total_samples = total_samples + len(sub_samples)\n","      samples_summary_dict[subfolder] = total_samples\n","  print('samples summary:', samples_summary_dict)\n","  return samples_dict, samples_summary_dict"],"metadata":{"id":"o_AFCYTkh3Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# samples_config = {\n","#     'sample_duration' : 15,\n","#     'overlap_factor' : 0.25,\n","#     'subfolders_ind' : [0, 1, 2, 3],\n","#     'min_samples' : 10\n","# }\n","# # sample_durations = [25]\n","# sample_durations = [3, 5, 10, 15, 20, 25]\n","# overlaps = [0, 0.25]\n","# for dur in sample_durations:\n","#   print('Sample Duration = ', dur)\n","#   samples_config['sample_duration'] = dur\n","#   for overlap in overlaps:\n","#     print('Overlap = ', overlap)\n","#     samples_config['overlap_factor'] = overlap\n","\n","#     samples_dict, samples_summary_dict = extract_samples(folder, samples_config)\n","#     save_path = './Samples/samples_'+ str(dur) + 'ol' + str(overlap)\n","#     with open(save_path , 'wb') as f:\n","#         pickle.dump((samples_dict, samples_summary_dict), f)"],"metadata":{"id":"Suibu0DoFqWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helper transformations and feature functions (temp - to be moved to file)"],"metadata":{"id":"cA3lSWL86zUn"}},{"cell_type":"code","source":["def return_span(series):\n","  return series.max() - series.min()"],"metadata":{"id":"zCbytSxp69qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def phi_theta_ratio(tt, data_theta, data_phi, pt_ratio_quants, dt=1, time_span = 3):\n","  \"\"\"\n","  \"\"\"\n","  new_tt = np.arange(tt[0], tt[-1]+dt, dt)\n","\n","  interp_data_theta = np.interp(new_tt, tt, data_theta)\n","  pd_data_theta = pd.Series(interp_data_theta)\n","  rolling_data_theta = pd_data_theta.rolling(time_span)\n","  span_theta = rolling_data_theta.apply(return_span)\n","\n","  interp_data_phi = np.interp(new_tt, tt, data_phi)\n","  pd_data_phi = pd.Series(interp_data_phi)\n","  rolling_data_phi = pd_data_phi.rolling(time_span)\n","  span_phi = rolling_data_phi.apply(return_span)\n","  span_ratios = np.arctan(span_phi/span_theta)\n","  span_ratios = span_ratios[~np.isnan(span_ratios)]\n","  span_ratios_quant = np.quantile(span_ratios, pt_ratio_quants)\n","\n","  return span_ratios.min(), span_ratios.max(), span_ratios_quant"],"metadata":{"id":"rr00aVkVePXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def PCA_angles_transformation(theta, phi):\n","  \"\"\"returns the data, rotated so that the principle axis of the data in the xy plane is on x\n","  \"\"\"\n","  tp_data = np.stack([theta, phi]).T\n","\n","  pca = PCA(n_components=2, svd_solver='full')\n","  angles_projected = pca.fit_transform(tp_data)\n","\n","  return angles_projected[:,0], angles_projected[:,1]\n","  # new_theta, new_phi = PCA_angles_transformation(cleaned_theta, cleaned_phi)"],"metadata":{"id":"gDei3W0x7_-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def angle_span_ratios(theta, phi):\n","  orig_ratio = (phi.max() - phi.min())/(theta.max() - theta.min())\n","  pca_theta, pca_phi = PCA_angles_transformation(theta, phi)\n","  pca_ratio = (pca_phi.max() - pca_phi.min())/(pca_theta.max() - pca_theta.min())\n","  return orig_ratio, pca_ratio"],"metadata":{"id":"fxOjmDRV_Q2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def smoothed(data, window = 5):\n","  \"\"\"\n","  Args:\n","    data: A one dimensional array of data.\n","    window: The window of the convolution used for low-pass filtering.\n","\n","  Returns:\n","    A smoothed version of the data\n","  \"\"\"\n","  # Run a low pass filter.\n","  padded_data = np.pad(data, (window//2, window//2), 'edge')\n","  smooth_data = np.convolve(padded_data, np.ones(window) / window, mode='valid')\n","  return smooth_data"],"metadata":{"id":"9PA7CITmDlvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract features"],"metadata":{"id":"73loRkCF7vye"}},{"cell_type":"code","source":["  t1 = np.arange(2, 5, 0.04)\n","  t2 = np.arange(5, 6, 0.04)\n","  part1 = (0.5/5**2)*t1**2 + 0.5\n","  part2 = 1 - 0.5*(t2-5)\n","  filt1 = np.flip(np.hstack([part1, part2]))\n","  t4 = np.arange(-5, -2, 0.04)\n","  t3 = np.arange(-6, -5, 0.04)\n","  part3 = -1 - 0.5*(t3+5)\n","  part4 = -(0.5/5**2)*t4**2 - 0.5\n","  filt2 = np.flip(np.hstack([part3, part4]))"],"metadata":{"id":"IoV6bPfHbZ6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(sample):\n","  tt, xx, yy, zz, size_hor, size_ver = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5]\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  # Basic transformations\n","  #standerdizing to start from 0\n","  phi = phi - phi[0]\n","  theta = theta - theta[0]\n","  # clean\n","  # xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999, replace_by = 'med')\n","\n","  # interpolate\n","  delta = 0.04\n","  interp_tt, interp_theta = interpolate_data(tt, theta, dt=delta, fixed = False)\n","  interp_tt, interp_phi = interpolate_data(tt, phi, dt=delta, fixed = False)\n","  # interp_tt, interp_xx = interpolate_data(tt, xx, dt=delta, fixed = False)\n","  # interp_tt, interp_yy = interpolate_data(tt, yy, dt=delta, fixed = False)\n","  # interp_tt, interp_zz = interpolate_data(tt, zz, dt=delta, fixed = False)\n","  interp_tt, interp_size_hor = interpolate_data(tt, size_hor, dt=delta, fixed = False)\n","  interp_tt, interp_size_ver = interpolate_data(tt, size_ver, dt=delta, fixed = False)\n","  fix_interp_tt, fix_interp_theta = interpolate_data(tt, theta, dt=delta, fixed = True)\n","  fix_interp_tt, fix_interp_phi = interpolate_data(tt, phi, dt=delta, fixed = True)\n","\n","  # derive\n","  interp_vel_theta, interp_vel_phi, interp_t_vel = derive(interp_theta, interp_phi, interp_tt)\n","  vel_theta, vel_phi, t_vel = derive(theta, phi, tt)\n","  abs_vel = np.sqrt(vel_theta**2 + vel_phi**2)\n","  acc_theta, acc_phi, t_acc = derive(vel_theta, vel_phi, t_vel)\n","  abs_acc = np.sqrt(acc_theta**2 + acc_phi**2)\n","  fix_vel_theta, fix_vel_phi, t_vel = derive(fix_interp_theta, fix_interp_phi, fix_interp_tt)\n","\n","  # rolling window smoothing\n","  s_theta = smoothed(theta, window = 5)\n","  s_phi = smoothed(phi, window = 5)\n","  delta = 0.04\n","  # interp_tt, s_interp_theta = interpolate_data(tt, s_theta, dt=delta, fixed = False)\n","  # interp_tt, s_interp_phi = interpolate_data(tt, s_phi, dt=delta, fixed = False)\n","  # s_vel_theta = np.diff(s_interp_theta)/np.diff(interp_tt)\n","  # s_vel_phi = np.diff(s_interp_phi)/np.diff(interp_tt)\n","  # s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_acc_theta, s_acc_phi, s_t_acc = derive(s_vel_theta, s_vel_phi, s_t_vel)\n","  s_abs_acc = np.sqrt(s_acc_theta**2 + s_acc_phi**2)\n","\n","  # FEATURES\n","  returning_features = []\n","  feature_names = []\n","  # extreme values (scale dependent)\n","  max_size_hor = size_hor.max()\n","  max_size_ver = size_ver.max()\n","  max_elevation = phi.max()\n","  min_elevation = phi.min()\n","  max_theta_vel =  np.abs(vel_theta).max()\n","  max_phi_vel =  np.abs(vel_phi).max()\n","  max_vel = abs_vel.max()\n","  max_theta_acc =  np.abs(acc_theta).max()\n","  max_phi_acc =  np.abs(acc_phi).max()\n","  max_acc = abs_acc.max()\n","  returning_features.extend([max_elevation, min_elevation, max_theta_vel, max_phi_vel, max_vel, max_theta_acc, max_phi_acc, max_acc])\n","  feature_names.extend(['max_elevation', 'min_elevation', 'max_theta_vel', 'max_phi_vel', 'max_vel', 'max_theta_acc', 'max_phi_acc', 'max_acc'])\n","\n","  s_max_elevation = s_phi.max()\n","  s_min_elevation = s_phi.min()\n","  s_max_theta_vel = np.abs(s_vel_theta).max()\n","  s_max_phi_vel = np.abs(s_vel_phi).max()\n","  s_max_vel = s_abs_vel.max()\n","  s_med_vel = np.median(s_abs_vel)\n","  s_max_theta_acc = np.abs(s_acc_theta).max()\n","  s_max_phi_acc = np.abs(s_acc_phi).max()\n","  s_max_acc = s_abs_acc.max()\n","  s_med_acc = np.median(s_abs_acc)\n","  returning_features.extend([s_max_elevation, s_min_elevation, s_max_theta_vel, s_max_phi_vel, s_max_vel, s_max_theta_acc, s_max_phi_acc, s_max_acc])\n","  feature_names.extend(['s_max_elevation', 's_min_elevation', 's_max_theta_vel', 's_max_phi_vel', 's_max_vel', 's_max_theta_acc', 's_max_phi_acc', 's_max_acc'])\n","\n","  # statistics and noise\n","  theta_std = local_std(interp_theta, window = 10)\n","  phi_std = local_std(interp_phi, window = 10)\n","  theta_vel_std = local_std(interp_vel_theta, window = 10)\n","  phi_vel_std = local_std(interp_vel_phi, window = 10)\n","  returning_features.extend([theta_std, phi_std, theta_vel_std, phi_vel_std])\n","  feature_names.extend(['theta_std', 'phi_std', 'theta_vel_std', 'phi_vel_std'])\n","\n","  # span\n","  theta_span = theta.max() - theta.min()\n","  phi_span = phi.max() - phi.min()\n","  returning_features.extend([theta_span, phi_span])\n","  feature_names.extend(['theta_span', 'phi_span'])\n","\n","  # avg values\n","  med_elevation = np.median(phi)\n","  med_size_hor = np.median(size_hor)\n","  med_size_ver = np.median(size_ver)\n","  med_theta_vel =  np.median(vel_theta)\n","  med_phi_vel =  np.median(vel_phi)\n","  med_theta_acc =  np.median(acc_theta)\n","  med_phi_acc =  np.median(acc_phi)\n","  returning_features.extend([med_elevation, med_theta_vel, med_phi_vel, med_theta_acc, med_phi_acc, s_med_vel, s_med_acc])\n","  feature_names.extend(['med_elevation', 'med_theta_vel', 'med_phi_vel', 'med_theta_acc', 'med_phi_acc', 's_med_vel', 's_med_acc'])\n","\n","  # Bounding box data\n","  returning_features.extend([max_size_hor, max_size_ver, med_size_hor, med_size_ver])\n","  feature_names.extend(['max_size_hor', 'max_size_ver', 'med_size_hor', 'med_size_ver'])\n","\n","\n","  # correlations\n","  # print(np.any(np.isinf(vel_theta)), np.any(np.isinf(vel_phi)))\n","  ascending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.max(), filt1, mode='valid'))\n","  decending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.min(), filt2, mode='valid'))\n","\n","  pt_vel_corr, _ = pearsonr(np.abs(vel_theta), np.abs(vel_phi))\n","  pt_acc_corr, _ = pearsonr(np.abs(acc_theta), np.abs(acc_phi))\n","\n","  returning_features.extend([ascending_sig, decending_sig, pt_vel_corr, pt_acc_corr])\n","  feature_names.extend(['ascending_sig', 'decending_sig', 'pt_vel_corr', 'pt_acc_corr'])\n","\n","  # FFT\n","\n","  # yf = fft(fix_vel_phi)\n","  # yf_trimmed_abs = np.abs(yf[0:len(yf)//2+1])\n","\n","  # ratios\n","  orig_ratio, pca_ratio = angle_span_ratios(theta, phi)\n","  std_vs_span = np.sqrt((theta_std**2 + phi_std**2)/(theta_span**2 + phi_span**2))\n","  returning_features.extend([orig_ratio, pca_ratio, std_vs_span])\n","  feature_names.extend(['orig_ratio', 'pca_ratio', 'std_vs_span'])\n","\n","  # scale independent features\n","  # measuring geometric curvature\n","\n","  discrete_angle_dist = np.sqrt(np.diff(interp_theta)**2 + np.diff(interp_phi)**2)\n","  cumsum_angles_dist = np.cumsum(discrete_angle_dist)\n","  path_total_length = cumsum_angles_dist[-1]\n","  cum_dist = np.hstack([0, cumsum_angles_dist])\n","\n","  angles = np.stack([interp_theta, interp_phi]).T\n","  eucl_dist = cdist(angles, angles, 'euclidean') + np.ones([len(interp_theta), len(interp_theta)])*1e-6 #added to avoid devision by 0\n","\n","  path_dist = np.abs(cum_dist[:, None] - cum_dist)\n","  dist_ratio = path_dist/eucl_dist\n","  distance_buffer = 5 #removing ratios of points close to one another, since this may be noisy\n","  all_curve_ratios = sum((dist_ratio[i,i+distance_buffer:].tolist() for i in range(dist_ratio.shape[0])), [])\n","  curve_quants = np.arange(0.1, 0.95, 0.1)\n","  curve_quantiles = np.quantile(all_curve_ratios, curve_quants)\n","  max_curve_ratio = np.max(all_curve_ratios)\n","  returning_features.extend([*curve_quantiles, max_curve_ratio])\n","  q_names = ['curve quant ' + str(round(q, 2)) for q in curve_quants]\n","  feature_names.extend(q_names + ['max_curve_ratio'])\n","\n","  # measuring angles ratios (quantiles):\n","  # e.g. object rising fast without changing azimuth, or changing azimuth without elevation\n","  pt_ratio_quants = np.arange(0.1, 0.95, 0.1)\n","  min_span_ratios, max_span_ratios, span_ratios_quant = phi_theta_ratio(tt, theta, phi, pt_ratio_quants, dt=1, time_span = 3)\n","  returning_features.extend([*span_ratios_quant, max_span_ratios, min_span_ratios])\n","  q_names = ['pt ratio quant ' + str(round(q, 2)) for q in pt_ratio_quants]\n","  feature_names.extend(q_names + ['max_span_ratios', 'min_span_ratios'])\n","\n","  # velocity and acceleration profiles (ratios)\n","  motion_quants = np.arange(0.1, 0.95, 0.1)\n","  vel_quantiles = np.quantile(s_abs_vel, motion_quants)\n","  vel_profile = vel_quantiles/s_med_vel\n","  acc_quantiles = np.quantile(s_abs_acc, motion_quants)\n","  acc_profile = acc_quantiles/s_med_acc\n","  q_vel_names = ['vel quant ' + str(round(q, 2)) for q in motion_quants]\n","  q_acc_names = ['acc quant ' + str(round(q, 2)) for q in motion_quants]\n","  returning_features.extend([*vel_profile, *acc_profile])\n","  feature_names.extend(q_vel_names + q_acc_names)\n","\n","  return returning_features, feature_names"],"metadata":{"id":"yEl_k3nM3aEf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def prepare_dataset(split_config):\n","  print('collecting samples')\n","  # subfolders_list = [subf_dict[i] for i in split_config['subfolders_ind']]\n","  subfolders_list = [subf_dict[i] for i in np.arange(4)]\n","  save_path = './Samples/samples_'+ str(split_config['sample_duration']) + 'ol' + str(split_config['overlap_factor'])\n","  with open(save_path , 'rb') as f:\n","    samples_dict, samples_summary_dict = pickle.load(f)\n","\n","  # files_labels = []\n","  all_samples = []\n","  samples_labels = []\n","  samples_filenames = []\n","  samples_light = []\n","  database_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","    subfolder_path = os.path.join(folder, subfolder)\n","    files = os.listdir(subfolder_path) # Can also be taken from samples_dict keys\n","    # files_labels.extend([labels_dict[subfolder]]*len(files))\n","    n_subfolder_samples = 0\n","    for file in files:\n","      light_domain = file[:3]\n","      sub_samples = samples_dict[file]\n","      all_samples.extend(sub_samples)\n","      sub_labels = [labels_dict[subfolder]]*len(sub_samples)\n","      samples_labels.extend(sub_labels)\n","      sub_light = [light_domain]*len(sub_samples)\n","      samples_light.extend(sub_light)\n","      sub_file = [file]*len(sub_samples)\n","      samples_filenames.extend(sub_file)\n","      n_subfolder_samples = n_subfolder_samples + len(sub_samples)\n","    database_summary_dict[subfolder] = n_subfolder_samples\n","  print('extracting features')\n","  data = []\n","  for i, sample in enumerate(all_samples):\n","    data_row, features = extract_features(sample)\n","    data.append(data_row)\n","\n","  df = pd.DataFrame(data, columns=features)\n","  df['light'] = samples_light\n","  df['file'] = samples_filenames\n","  df['label'] = samples_labels\n","\n","  print('Done')\n","  return df, features"],"metadata":{"id":"jdQPqKaEdya_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split"],"metadata":{"id":"XFURuAe4rRag"}},{"cell_type":"code","source":["def split_by_files(df, df0, split_config):\n","  files = np.unique(df['file'].values)\n","  files_ind = [np.where(df['file'].values == f)[0][0] for f in files]\n","  files_labels = df['label'].values[files_ind]\n","  rs = split_config['random_state']\n","  ts = split_config['test_split_files']\n","  files_train, files_test = train_test_split(files, test_size=ts, random_state=rs, stratify=files_labels)\n","  train_df = df[df['file'].isin(files_train)]\n","  test_df = df0[df0['file'].isin(files_test)]\n","  return train_df, test_df"],"metadata":{"id":"fMADz8abAfCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(labels_dict)\n","print(subf_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04l2VYPKQRUF","executionInfo":{"status":"ok","timestamp":1705958359734,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"47c9be9b-9203-499b-ef3a-9b8e0ed28f80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}\n","{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}\n"]}]},{"cell_type":"code","source":["subf_dict[3] = 'static-object'"],"metadata":{"id":"yjSqkrE_7P29"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"aRHy7rCLBl73"}},{"cell_type":"code","source":["def evaluate(labels, predictions, caption, plot_cm , print_scores):\n","\n","  classes = np.union1d(labels, predictions)\n","  tick_names = [subf_dict[c] for c in classes]\n","  # Confusion Matrix - Multi class\n","  cm = confusion_matrix(labels, predictions)\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                               display_labels=tick_names)\n","  if plot_cm:\n","    disp.plot()\n","    disp.ax_.set_title(caption)\n","    plt.show()\n","\n","  # Scores #\n","  # f1_s = f1_score(labels, predictions, sample_weight=None, average = 'weighted', zero_division='warn')\n","  report = classification_report(labels, predictions, target_names = tick_names, output_dict=True, digits=3, zero_division = 0)\n","\n","  f1_all = report['weighted avg']['f1-score']\n","\n","  if print_scores == True:\n","      print(classification_report(labels, predictions, target_names = tick_names,digits=3, zero_division = 0))\n","      # print(f1_s)\n","      print('Average F1 score for all classes = ', f1_all)\n","      print('------------------------')\n","      print('UAV report')\n","      print('------------------------')\n","      print('precision = ', report['uav']['precision'])\n","      print('recall = ', report['uav']['recall'])\n","      print('F1 = ', report['uav']['f1-score'])\n","      print('support = ', report['uav']['support'])\n","\n","  return report"],"metadata":{"id":"OcD3qzt_FUIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experiments"],"metadata":{"id":"ngvs1istUO2a"}},{"cell_type":"code","source":["# record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","# exp_record = pd.DataFrame(columns = record_columns)"],"metadata":{"id":"C-JfJdNhbq-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conf2str(split_config, features_config):\n","  conf_str = 'sd' + str(split_config['sample_duration']) \\\n","  +'of' + str(split_config['overlap_factor']) \\\n","  +'ms' + str(split_config['min_samples']) \\\n","  +'f_date' + str(features_config['date'])\n","  return conf_str"],"metadata":{"id":"rTu7JS3JMywC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_dict = {\n","    'extremum' : np.arange(0,16),\n","    'std' : np.arange(16,20),\n","    'span' : np.arange(20,22),\n","    'med' : np.arange(22,29),\n","    'bbox' : np.arange(29,33),\n","    'corr_sig' : np.arange(33,37),\n","    'full_ratio' : np.arange(37,40),\n","    'curve' : np.arange(40,50),\n","    'pt_ratio' : np.arange(50,61),\n","    'vel_acc_profile' : np.arange(61,79)\n","}"],"metadata":{"id":"XoqR7TgInQyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets(split_config, features_config):\n","  conf_str = conf2str(split_config, features_config)\n","  database_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database_path):\n","    with open(database_path , 'rb') as f:\n","      df, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df, features = prepare_dataset(split_config)\n","    with open(database_path , 'wb') as f:\n","      pickle.dump((df, features), f)\n","\n","  #retrieving 0 overlap matching df\n","  ol0_config = split_config.copy()\n","  ol0_config['overlap_factor'] = 0\n","  conf_str = conf2str(ol0_config, features_config)\n","\n","  database0_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database0_path):\n","    with open(database0_path , 'rb') as f:\n","      df0, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df0, features = prepare_dataset(ol0_config)\n","    with open(database0_path , 'wb') as f:\n","      pickle.dump((df0, features), f)\n","  return df, df0, features"],"metadata":{"id":"AeEmiAjM7akc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","exp_record = pd.DataFrame(columns = record_columns)\n","\n","split_config = {\n","    'sample_duration' : 15,\n","    'overlap_factor' : 0.25,\n","    'subfolders_ind' : [0, 1, 2, 3],\n","    'min_samples' : 10,\n","    'random_state' : 42,\n","    'test_split_files' : 0.2, # 2 for n files\n","    'kfold' : 5\n","  }\n","\n","features_config = {\n","    'date' : 20_1_2024,\n","    'extremum' : True,\n","    'std' : True,\n","    'span' : True,\n","    'med' : True,\n","    'bbox' : True,\n","    'corr_sig' : True,\n","    'full_ratio' : True,\n","    'curve' : True,\n","    'pt_ratio' : True,\n","    'vel_acc_profile' : True\n","  }\n","\n","xgb_config = {\n","    'max_depth' : 6,\n","    'model_random_state' : 42,\n","    'eta' : 0.3\n","  }\n","\n","# split_random_states = [32]\n","# sample_durations = [20]\n","# max_depth = [10]\n","\n","model_random_states = [42]\n","split_random_states = [42]\n","sample_durations = [5, 10, 15, 20, 25]\n","# sample_durations = [10]\n","max_depth = [2, 4, 7, 10]\n","etas = [0.05, 0.1, 1, 3]\n","\n","# max_depth = [4]\n","# etas = [1]\n","\n","model_name = 'XGB'\n","model_config = xgb_config\n","now = datetime.now()\n","date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","for rs in split_random_states:\n","  split_config['random_state'] = rs\n","  for dur in sample_durations:\n","    split_config['sample_duration'] = dur\n","    for md in max_depth:\n","      model_config['max_depth'] = md\n","      for eta in etas:\n","        model_config['eta'] = eta\n","        # print(split_config)\n","        # print(model_config)\n","        #Don't change split_config beyond this point\n","        df, df0, features = get_datasets(split_config, features_config)\n","        train_df, test_df = split_by_files(df, df0, split_config)\n","        #patch!!!\n","        train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","        test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","\n","        feature_columns = np.empty(0)\n","        for key in features_config.keys():\n","          if features_config[key] == True:\n","            feature_columns = np.hstack([feature_columns, features_dict[key]])\n","\n","        # train_df = train_df_0.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","        # train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","        train_df = train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","        train = train_df.iloc[:, feature_columns].values\n","        # test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","        test = test_df.iloc[:, feature_columns].values\n","        train_labels = train_df['label'].values\n","        test_labels = test_df['label'].values\n","        train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=train_labels)\n","\n","        #kfold cross validation\n","        # splitting needs to be done according to filenames to not have train and val samples from the same file\n","        files = np.unique(train_df['file'].values)\n","        files_ind = [np.where(train_df['file'].values == f)[0][0] for f in files]\n","        files_labels = train_df['label'].values[files_ind]\n","        # use stratified Kfold for imbalance handling\n","        skf = StratifiedKFold(n_splits=split_config['kfold'])\n","        skfold_ind = skf.split(files, files_labels)\n","        uav_precision = []\n","        uav_recall = []\n","        uav_f1 = []\n","        weighted_f1 = []\n","        for (train_ind, val_ind) in skfold_ind:\n","          train_files = files[train_ind]\n","          k_train_df = train_df[train_df['file'].isin(train_files)]\n","          k_train_df = k_train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","          k_train_labels = k_train_df['label'].values\n","          k_train = k_train_df.iloc[:, feature_columns].values\n","          k_train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=k_train_labels)\n","\n","          val_files = files[val_ind]\n","          k_val_df = df0[df0['file'].isin(val_files)] # taking calculations from samples without overlap\n","          k_val_df = k_val_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","          k_val_labels = k_val_df['label'].values\n","          k_val = k_val_df.iloc[:, feature_columns].values\n","\n","          if model_name == 'XGB':\n","            xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=k_train_classes_weights)\n","            xgb_model.fit(k_train, k_train_labels)\n","            k_predictions = xgb_model.predict(k_val)\n","\n","          k_report = evaluate(k_val_labels, k_predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","          uav_precision.append(k_report['uav']['precision'])\n","          uav_recall.append(k_report['uav']['recall'])\n","          uav_f1.append(k_report['uav']['f1-score'])\n","          weighted_f1.append(k_report['weighted avg']['f1-score'])\n","\n","        uav_precision_m = sum(uav_precision)/len(uav_precision)\n","        uav_recall_m = sum(uav_recall)/len(uav_recall)\n","        uav_f1_m = sum(uav_f1)/len(uav_f1)\n","        weighted_f1_m = sum(weighted_f1)/len(weighted_f1)\n","\n","        # print('Fitting all train and test evaluation')\n","        if model_name == 'XGB':\n","          xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=train_classes_weights)\n","          xgb_model.fit(train, train_labels)\n","          predictions = xgb_model.predict(test)\n","          test_report = evaluate(test_labels, predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","\n","        # wrong_ind = np.where(predictions != test_labels)\n","        # print('wrong on:')\n","        # print(test_df['file'].iloc[wrong_ind])\n","        test_files = np.unique(test_df['file'])\n","        exp_record.loc[len(exp_record)] = [date_time, dur, test_files, split_config.copy(), features_config, model_name, model_config.copy(),\n","                          test_report, uav_precision_m, uav_recall_m, uav_f1_m, weighted_f1_m]"],"metadata":{"id":"QfM7V3qc6I8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# features"],"metadata":{"id":"yMxHO201ojBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["date_time_ = str(date_time).replace(\"/\",\"_\")\n","results_path = './Features Extraction Results/features_extraction_results_' + date_time_\n","\n","with open(results_path , 'wb') as f:\n","  pickle.dump(exp_record, f)"],"metadata":{"id":"MRJtp4JPMCVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_record"],"metadata":{"id":"3xPTz6ogGTQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_record.iloc[0]['Evaluation Report']"],"metadata":{"id":"XoN0aN2hjH5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# d_tree = tree.DecisionTreeClassifier(max_depth=3, random_state=rs)\n","# d_tree.fit(train, train_labels)\n","# predictions = d_tree.predict(test)\n","# report = evaluate(test_labels, predictions, model_name+'_depth_'+str(model_config['max_depth']), True , True)"],"metadata":{"id":"k5soQgdVEYED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tree.plot_tree(d_tree)"],"metadata":{"id":"M1Rz-7qZFeOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def experiment():\n","#   train_df, test_df = prepare_dataset(folder, split_config, features_config)\n","#   now = datetime.now()\n","#   date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","#   train_df = train_df.sample(frac = 1) #shuffled df\n","#   train = train_df.iloc[:,:-2].values\n","#   test = test_df.iloc[:,:-2].values\n","#   train_labels = train_df.iloc[:,-1].values\n","#   test_labels = test_df.iloc[:,-1].values"],"metadata":{"id":"PFEu0xuyj_7X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####SVM"],"metadata":{"id":"ls7ArhkzFoPY"}},{"cell_type":"code","source":["# svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n","# svm.fit(data, labels)"],"metadata":{"id":"bFG_yQltAwi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# svm.predict(test)"],"metadata":{"id":"uVjN3rQgBa6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_labels"],"metadata":{"id":"meOPWM-iDdxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Random Forest"],"metadata":{"id":"TNwNF9LuFuPy"}},{"cell_type":"code","source":["# rf = RandomForestClassifier(max_depth=7, random_state=0)\n","# rf.fit(data, labels)"],"metadata":{"id":"qmTt46-NFck4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predictions = rf.predict(test)\n","# print(predictions)"],"metadata":{"id":"KL2erMtDF_k_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test_labels = [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 4, 4, 4, 4, 4]\n","# predictions = [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 4, 4, 4, 4, 4]"],"metadata":{"id":"j-Bk2zXZiMGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# report = evaluate(test_labels, predictions, 'test_evaluation', True , True)"],"metadata":{"id":"2L0XYI84MLb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# report['uav']"],"metadata":{"id":"CetHzW1lPUYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample visualization"],"metadata":{"id":"QtjE8uLPBbED"}},{"cell_type":"code","source":["def sample_visualization(sample, start = 0, finish = -999):\n","  \"\"\"\n","  Plots the different preprocessing stages for a single file\n","  Zoom into the plot by specifying the start and end indices\n","  \"\"\"\n","  tt, xx, yy, zz, size_hor, size_ver = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5]\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","\n","  if finish == -999:\n","      start = 0\n","      finish = len(tt)\n","\n","  fig = plt.figure(figsize = [10,10])\n","  ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n","  ax1.plot(xx[start:finish], yy[start:finish], zz[start:finish], '.-b', label='Position')\n","  ax1.set_title('Raw xyz Data')\n","  ax1.set_xlabel('x')\n","  ax1.set_ylabel('y')\n","  ax1.set_zlabel('z')\n","  plt.legend()\n","  #plot cleaned data\n","  cleaned_theta, cleaned_phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","\n","  ax2 = fig.add_subplot(2, 2, 2)\n","  ax2.plot(cleaned_theta[start:finish], cleaned_phi[start:finish], '.-g', label='Angles')\n","  ax2.set_title('Cleaned Angles Data')\n","  ax2.set_xlabel('Azimuth [deg]')\n","  ax2.set_ylabel('Elevation [deg]')\n","  plt.legend()\n","  #plot interpolation\n","  delta = 0.04\n","  new_tt, interp_theta = interpolate_data(tt, cleaned_theta, dt=delta, fixed = False)\n","  new_tt, interp_phi = interpolate_data(tt, cleaned_phi, dt=delta, fixed = False)\n","\n","  #to plot the same span as not interpolated - find the right time marker\n","  new_finish = np.nonzero(new_tt>=tt[finish-1])[0][0]\n","  ax3 = fig.add_subplot(2, 2, 3)\n","  ax3.plot(interp_theta[start:new_finish], interp_phi[start:new_finish], '.-r', label='Interploated angles')\n","  ax3.set_title('Interpolated Data')\n","  ax3.set_xlabel('Azimuth [deg]')\n","  ax3.set_ylabel('Elevation [deg]')\n","  plt.legend()\n","  #plot elevation derivative\n","  # vel_theta = np.diff(interp_theta)/np.diff(new_tt)\n","  vel_phi = np.diff(interp_phi)/np.diff(new_tt)\n","  t_vel = (new_tt[:-1] + new_tt[1:])/2\n","\n","  ax4 = fig.add_subplot(2, 2, 4)\n","  ax4.plot(t_vel[start:new_finish-1], vel_phi[start:new_finish-1], '.-m', label='vel phi')\n","  ax4.set_title('Elevation Derivative')\n","  ax4.set_xlabel('Azimuth [deg]')\n","  ax4.set_ylabel('Elevation [deg]')\n","  plt.legend()"],"metadata":{"id":"JEa3smf2_xb_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1r7p9VanU_m81qruh02RxfDhdsHbCnxHh","timestamp":1714383724037},{"file_id":"1r3QlF2dWJaA7LVwNG0NcHRBC7RAv-28R","timestamp":1704878410179},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"toc_visible":true,"authorship_tag":"ABX9TyMkf9GweoJ+v0Wc9C2ryZ+s"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
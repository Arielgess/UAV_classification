{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **Overview**\n","\n","This notebooks contains tools for preprocessing and visualization of the Walaris dataset for flight trajectories, given as Json files.\n"]},{"cell_type":"markdown","source":["### To Do\n"],"metadata":{"id":"AbMGQuI36K3c"}},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25341,"status":"ok","timestamp":1714645153333,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"},"user_tz":-180},"id":"q2hR_rZ4hcwY","outputId":"ed9e7bfa-17ee-4be8-e6b6-8e6e171434e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","from sklearn.utils import class_weight\n","import xgboost as xgb\n","\n","\n","# prompt: Access a folder on google drive and import the data locally\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1714645153333,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"},"user_tz":-180},"id":"EP0cLHeSozdF","outputId":"2313a41e-f2db-4849-b341-6787a5a284e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/bird/VIS_bird_20230605-406_20230605-142825.json\"\n","# file_path = \"track_data/bird/VIS_bird_20231024-22251_20231024-083633.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231023-6210_20231023-162937.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231025-21925_20231025-074948.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10903_20231027-114017.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-11132_20231027-115333.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10582_20231027-113033.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231030-6113_20231030-140702.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10437_20231027-112435.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# plot_vs_time(file_path, 'vel')"],"metadata":{"id":"TNVFI13GYV1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Segmentation"],"metadata":{"id":"EmuJtXUpZujJ"}},{"cell_type":"markdown","source":["Assume interpolation of dt = 40 msec.\\\n","For example: a sequence of 5 seconds is equivalent to 25*5 = 125"],"metadata":{"id":"RleKlQDgaQHZ"}},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","  sample_duration = samples_config['sample_duration']\n","  overlap_factor = samples_config['overlap_factor']\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","  complete_sample = np.stack([tt, xx, yy, zz, size_hor, size_ver]).T\n","\n","  #segmenting\n","  skip = sample_duration * (1-overlap_factor)\n","  current_index = [0]\n","  end_index = np.nonzero(tt>=tt[0]+sample_duration)[0]\n","\n","  sub_samples = []\n","  while end_index.size:\n","      # print(current_index[0], end_index[0])\n","      if end_index[0] - current_index[0] > samples_config['min_samples']: ### a threshold for a minimum number of datapoints in a sample\n","        sub_samples.append(complete_sample[current_index[0]:end_index[0], :])\n","      end_index = np.nonzero(tt>=tt[current_index[0]]+skip+sample_duration)[0]\n","      current_index = np.nonzero(tt>=tt[current_index[0]]+skip)[0]\n","\n","  return sub_samples, light_domain"],"metadata":{"id":"RL5aYuYSd0v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["subf_dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6KbmQvmQbP3U","executionInfo":{"status":"ok","timestamp":1707207137511,"user_tz":-120,"elapsed":469,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"92b6b958-99d6-4ba3-d403-7e6988bbe6cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  subfolders = os.listdir(folder)\n","  subfolders_list = [subfolders[i] for i in samples_config['subfolders_ind']]\n","\n","  samples_dict = {}\n","  samples_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","      print(subfolder)\n","      total_samples = 0\n","      subfolder_path = os.path.join(folder, subfolder)\n","      files = os.listdir(subfolder_path)\n","      for file in files:\n","          file_path = os.path.join(subfolder_path, file)\n","          sub_samples, light_domain = segment_file_by_time(file_path, samples_config)\n","          samples_dict[file] = sub_samples\n","          total_samples = total_samples + len(sub_samples)\n","      samples_summary_dict[subfolder] = total_samples\n","  print('samples summary:', samples_summary_dict)\n","  return samples_dict, samples_summary_dict"],"metadata":{"id":"o_AFCYTkh3Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# samples_config = {\n","#     'sample_duration' : 15,\n","#     'overlap_factor' : 0.25,\n","#     'subfolders_ind' : [0, 1, 2, 3],\n","#     'min_samples' : 10\n","# }\n","# # sample_durations = [25]\n","# sample_durations = [3, 5, 10, 15, 20, 25]\n","# overlaps = [0, 0.25]\n","# for dur in sample_durations:\n","#   print('Sample Duration = ', dur)\n","#   samples_config['sample_duration'] = dur\n","#   for overlap in overlaps:\n","#     print('Overlap = ', overlap)\n","#     samples_config['overlap_factor'] = overlap\n","\n","#     samples_dict, samples_summary_dict = extract_samples(folder, samples_config)\n","#     save_path = './Samples/samples_'+ str(dur) + 'ol' + str(overlap)\n","#     with open(save_path , 'wb') as f:\n","#         pickle.dump((samples_dict, samples_summary_dict), f)"],"metadata":{"id":"Suibu0DoFqWA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Helper transformations and feature functions (temp - to be moved to file)"],"metadata":{"id":"cA3lSWL86zUn"}},{"cell_type":"code","source":["def return_span(series):\n","  return series.max() - series.min()"],"metadata":{"id":"zCbytSxp69qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def phi_theta_ratio(tt, data_theta, data_phi, pt_ratio_quants, dt=1, time_span = 3):\n","  \"\"\"\n","  \"\"\"\n","  new_tt = np.arange(tt[0], tt[-1]+dt, dt)\n","\n","  interp_data_theta = np.interp(new_tt, tt, data_theta)\n","  pd_data_theta = pd.Series(interp_data_theta)\n","  rolling_data_theta = pd_data_theta.rolling(time_span)\n","  span_theta = rolling_data_theta.apply(return_span)\n","\n","  interp_data_phi = np.interp(new_tt, tt, data_phi)\n","  pd_data_phi = pd.Series(interp_data_phi)\n","  rolling_data_phi = pd_data_phi.rolling(time_span)\n","  span_phi = rolling_data_phi.apply(return_span)\n","  span_ratios = np.arctan(span_phi/span_theta)\n","  span_ratios = span_ratios[~np.isnan(span_ratios)]\n","  span_ratios_quant = np.quantile(span_ratios, pt_ratio_quants)\n","\n","  return span_ratios.min(), span_ratios.max(), span_ratios_quant"],"metadata":{"id":"rr00aVkVePXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def PCA_angles_transformation(theta, phi):\n","  \"\"\"returns the data, rotated so that the principle axis of the data in the xy plane is on x\n","  \"\"\"\n","  tp_data = np.stack([theta, phi]).T\n","\n","  pca = PCA(n_components=2, svd_solver='full')\n","  angles_projected = pca.fit_transform(tp_data)\n","\n","  return angles_projected[:,0], angles_projected[:,1]\n","  # new_theta, new_phi = PCA_angles_transformation(cleaned_theta, cleaned_phi)"],"metadata":{"id":"gDei3W0x7_-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def angle_span_ratios(theta, phi):\n","  orig_ratio = (phi.max() - phi.min())/(theta.max() - theta.min())\n","  pca_theta, pca_phi = PCA_angles_transformation(theta, phi)\n","  pca_ratio = (pca_phi.max() - pca_phi.min())/(pca_theta.max() - pca_theta.min())\n","  return orig_ratio, pca_ratio"],"metadata":{"id":"fxOjmDRV_Q2r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def smoothed(data, window = 5):\n","  \"\"\"\n","  Args:\n","    data: A one dimensional array of data.\n","    window: The window of the convolution used for low-pass filtering.\n","\n","  Returns:\n","    A smoothed version of the data\n","  \"\"\"\n","  # Run a low pass filter.\n","  padded_data = np.pad(data, (window//2, window//2), 'edge')\n","  smooth_data = np.convolve(padded_data, np.ones(window) / window, mode='valid')\n","  return smooth_data"],"metadata":{"id":"9PA7CITmDlvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract features"],"metadata":{"id":"73loRkCF7vye"}},{"cell_type":"code","source":["  t1 = np.arange(2, 5, 0.04)\n","  t2 = np.arange(5, 6, 0.04)\n","  part1 = (0.5/5**2)*t1**2 + 0.5\n","  part2 = 1 - 0.5*(t2-5)\n","  filt1 = np.flip(np.hstack([part1, part2]))\n","  t4 = np.arange(-5, -2, 0.04)\n","  t3 = np.arange(-6, -5, 0.04)\n","  part3 = -1 - 0.5*(t3+5)\n","  part4 = -(0.5/5**2)*t4**2 - 0.5\n","  filt2 = np.flip(np.hstack([part3, part4]))"],"metadata":{"id":"IoV6bPfHbZ6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_features(sample):\n","  tt, xx, yy, zz, size_hor, size_ver = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5]\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  # Basic transformations\n","  #standerdizing to start from 0\n","  phi = phi - phi[0]\n","  theta = theta - theta[0]\n","  # clean\n","  # xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999, replace_by = 'med')\n","\n","  # interpolate\n","  delta = 0.04\n","  interp_tt, interp_theta = interpolate_data(tt, theta, dt=delta, fixed = False)\n","  interp_tt, interp_phi = interpolate_data(tt, phi, dt=delta, fixed = False)\n","  # interp_tt, interp_xx = interpolate_data(tt, xx, dt=delta, fixed = False)\n","  # interp_tt, interp_yy = interpolate_data(tt, yy, dt=delta, fixed = False)\n","  # interp_tt, interp_zz = interpolate_data(tt, zz, dt=delta, fixed = False)\n","  interp_tt, interp_size_hor = interpolate_data(tt, size_hor, dt=delta, fixed = False)\n","  interp_tt, interp_size_ver = interpolate_data(tt, size_ver, dt=delta, fixed = False)\n","  fix_interp_tt, fix_interp_theta = interpolate_data(tt, theta, dt=delta, fixed = True)\n","  fix_interp_tt, fix_interp_phi = interpolate_data(tt, phi, dt=delta, fixed = True)\n","\n","  # derive\n","  interp_vel_theta, interp_vel_phi, interp_t_vel = derive(interp_theta, interp_phi, interp_tt)\n","  vel_theta, vel_phi, t_vel = derive(theta, phi, tt)\n","  abs_vel = np.sqrt(vel_theta**2 + vel_phi**2)\n","  acc_theta, acc_phi, t_acc = derive(vel_theta, vel_phi, t_vel)\n","  abs_acc = np.sqrt(acc_theta**2 + acc_phi**2)\n","  fix_vel_theta, fix_vel_phi, t_vel = derive(fix_interp_theta, fix_interp_phi, fix_interp_tt)\n","\n","  # rolling window smoothing\n","  s_theta = smoothed(theta, window = 5)\n","  s_phi = smoothed(phi, window = 5)\n","  delta = 0.04\n","  # interp_tt, s_interp_theta = interpolate_data(tt, s_theta, dt=delta, fixed = False)\n","  # interp_tt, s_interp_phi = interpolate_data(tt, s_phi, dt=delta, fixed = False)\n","  # s_vel_theta = np.diff(s_interp_theta)/np.diff(interp_tt)\n","  # s_vel_phi = np.diff(s_interp_phi)/np.diff(interp_tt)\n","  # s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_acc_theta, s_acc_phi, s_t_acc = derive(s_vel_theta, s_vel_phi, s_t_vel)\n","  s_abs_acc = np.sqrt(s_acc_theta**2 + s_acc_phi**2)\n","\n","  # FEATURES\n","  returning_features = []\n","  feature_names = []\n","  # extreme values (scale dependent)\n","  max_size_hor = size_hor.max()\n","  max_size_ver = size_ver.max()\n","  max_elevation = phi.max()\n","  min_elevation = phi.min()\n","  max_theta_vel =  np.abs(vel_theta).max()\n","  max_phi_vel =  np.abs(vel_phi).max()\n","  max_vel = abs_vel.max()\n","  max_theta_acc =  np.abs(acc_theta).max()\n","  max_phi_acc =  np.abs(acc_phi).max()\n","  max_acc = abs_acc.max()\n","  returning_features.extend([max_elevation, min_elevation, max_theta_vel, max_phi_vel, max_vel, max_theta_acc, max_phi_acc, max_acc])\n","  feature_names.extend(['max_elevation', 'min_elevation', 'max_theta_vel', 'max_phi_vel', 'max_vel', 'max_theta_acc', 'max_phi_acc', 'max_acc'])\n","\n","  s_max_elevation = s_phi.max()\n","  s_min_elevation = s_phi.min()\n","  s_max_theta_vel = np.abs(s_vel_theta).max()\n","  s_max_phi_vel = np.abs(s_vel_phi).max()\n","  s_max_vel = s_abs_vel.max()\n","  s_med_vel = np.median(s_abs_vel)\n","  s_max_theta_acc = np.abs(s_acc_theta).max()\n","  s_max_phi_acc = np.abs(s_acc_phi).max()\n","  s_max_acc = s_abs_acc.max()\n","  s_med_acc = np.median(s_abs_acc)\n","  returning_features.extend([s_max_elevation, s_min_elevation, s_max_theta_vel, s_max_phi_vel, s_max_vel, s_max_theta_acc, s_max_phi_acc, s_max_acc])\n","  feature_names.extend(['s_max_elevation', 's_min_elevation', 's_max_theta_vel', 's_max_phi_vel', 's_max_vel', 's_max_theta_acc', 's_max_phi_acc', 's_max_acc'])\n","\n","  # statistics and noise\n","  theta_std = local_std(interp_theta, window = 10)\n","  phi_std = local_std(interp_phi, window = 10)\n","  theta_vel_std = local_std(interp_vel_theta, window = 10)\n","  phi_vel_std = local_std(interp_vel_phi, window = 10)\n","  returning_features.extend([theta_std, phi_std, theta_vel_std, phi_vel_std])\n","  feature_names.extend(['theta_std', 'phi_std', 'theta_vel_std', 'phi_vel_std'])\n","\n","  # span\n","  theta_span = theta.max() - theta.min()\n","  phi_span = phi.max() - phi.min()\n","  returning_features.extend([theta_span, phi_span])\n","  feature_names.extend(['theta_span', 'phi_span'])\n","\n","  # avg values\n","  med_elevation = np.median(phi)\n","  med_size_hor = np.median(size_hor)\n","  med_size_ver = np.median(size_ver)\n","  med_theta_vel =  np.median(vel_theta)\n","  med_phi_vel =  np.median(vel_phi)\n","  med_theta_acc =  np.median(acc_theta)\n","  med_phi_acc =  np.median(acc_phi)\n","  returning_features.extend([med_elevation, med_theta_vel, med_phi_vel, med_theta_acc, med_phi_acc, s_med_vel, s_med_acc])\n","  feature_names.extend(['med_elevation', 'med_theta_vel', 'med_phi_vel', 'med_theta_acc', 'med_phi_acc', 's_med_vel', 's_med_acc'])\n","\n","  # Bounding box data\n","  returning_features.extend([max_size_hor, max_size_ver, med_size_hor, med_size_ver])\n","  feature_names.extend(['max_size_hor', 'max_size_ver', 'med_size_hor', 'med_size_ver'])\n","\n","\n","  # correlations\n","  # print(np.any(np.isinf(vel_theta)), np.any(np.isinf(vel_phi)))\n","  ascending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.max(), filt1, mode='valid'))\n","  decending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.min(), filt2, mode='valid'))\n","\n","  pt_vel_corr, _ = pearsonr(np.abs(vel_theta), np.abs(vel_phi))\n","  pt_acc_corr, _ = pearsonr(np.abs(acc_theta), np.abs(acc_phi))\n","\n","  returning_features.extend([ascending_sig, decending_sig, pt_vel_corr, pt_acc_corr])\n","  feature_names.extend(['ascending_sig', 'decending_sig', 'pt_vel_corr', 'pt_acc_corr'])\n","\n","  # FFT\n","\n","  # yf = fft(fix_vel_phi)\n","  # yf_trimmed_abs = np.abs(yf[0:len(yf)//2+1])\n","\n","  # ratios\n","  orig_ratio, pca_ratio = angle_span_ratios(theta, phi)\n","  std_vs_span = np.sqrt((theta_std**2 + phi_std**2)/(theta_span**2 + phi_span**2))\n","  returning_features.extend([orig_ratio, pca_ratio, std_vs_span])\n","  feature_names.extend(['orig_ratio', 'pca_ratio', 'std_vs_span'])\n","\n","  # scale independent features\n","  # measuring geometric curvature\n","\n","  discrete_angle_dist = np.sqrt(np.diff(interp_theta)**2 + np.diff(interp_phi)**2)\n","  cumsum_angles_dist = np.cumsum(discrete_angle_dist)\n","  path_total_length = cumsum_angles_dist[-1]\n","  cum_dist = np.hstack([0, cumsum_angles_dist])\n","\n","  angles = np.stack([interp_theta, interp_phi]).T\n","  eucl_dist = cdist(angles, angles, 'euclidean') + np.ones([len(interp_theta), len(interp_theta)])*1e-6 #added to avoid devision by 0\n","\n","  path_dist = np.abs(cum_dist[:, None] - cum_dist)\n","  dist_ratio = path_dist/eucl_dist\n","  distance_buffer = 5 #removing ratios of points close to one another, since this may be noisy\n","  all_curve_ratios = sum((dist_ratio[i,i+distance_buffer:].tolist() for i in range(dist_ratio.shape[0])), [])\n","  curve_quants = np.arange(0.1, 0.95, 0.1)\n","  curve_quantiles = np.quantile(all_curve_ratios, curve_quants)\n","  max_curve_ratio = np.max(all_curve_ratios)\n","  returning_features.extend([*curve_quantiles, max_curve_ratio])\n","  q_names = ['curve quant ' + str(round(q, 2)) for q in curve_quants]\n","  feature_names.extend(q_names + ['max_curve_ratio'])\n","\n","  # measuring angles ratios (quantiles):\n","  # e.g. object rising fast without changing azimuth, or changing azimuth without elevation\n","  pt_ratio_quants = np.arange(0.1, 0.95, 0.1)\n","  min_span_ratios, max_span_ratios, span_ratios_quant = phi_theta_ratio(tt, theta, phi, pt_ratio_quants, dt=1, time_span = 3)\n","  returning_features.extend([*span_ratios_quant, max_span_ratios, min_span_ratios])\n","  q_names = ['pt ratio quant ' + str(round(q, 2)) for q in pt_ratio_quants]\n","  feature_names.extend(q_names + ['max_span_ratios', 'min_span_ratios'])\n","\n","  # velocity and acceleration profiles (ratios)\n","  motion_quants = np.arange(0.1, 0.95, 0.1)\n","  vel_quantiles = np.quantile(s_abs_vel, motion_quants)\n","  vel_profile = vel_quantiles/s_med_vel\n","  acc_quantiles = np.quantile(s_abs_acc, motion_quants)\n","  acc_profile = acc_quantiles/s_med_acc\n","  q_vel_names = ['vel quant ' + str(round(q, 2)) for q in motion_quants]\n","  q_acc_names = ['acc quant ' + str(round(q, 2)) for q in motion_quants]\n","  returning_features.extend([*vel_profile, *acc_profile])\n","  feature_names.extend(q_vel_names + q_acc_names)\n","\n","  return returning_features, feature_names"],"metadata":{"id":"yEl_k3nM3aEf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def prepare_dataset(split_config):\n","  print('collecting samples')\n","  # subfolders_list = [subf_dict[i] for i in split_config['subfolders_ind']]\n","  subfolders_list = [subf_dict[i] for i in np.arange(4)]\n","  save_path = './Samples/samples_'+ str(split_config['sample_duration']) + 'ol' + str(split_config['overlap_factor'])\n","  with open(save_path , 'rb') as f:\n","    samples_dict, samples_summary_dict = pickle.load(f)\n","\n","  # files_labels = []\n","  all_samples = []\n","  samples_labels = []\n","  samples_filenames = []\n","  samples_light = []\n","  database_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","    subfolder_path = os.path.join(folder, subfolder)\n","    files = os.listdir(subfolder_path) # Can also be taken from samples_dict keys\n","    # files_labels.extend([labels_dict[subfolder]]*len(files))\n","    n_subfolder_samples = 0\n","    for file in files:\n","      light_domain = file[:3]\n","      sub_samples = samples_dict[file]\n","      all_samples.extend(sub_samples)\n","      sub_labels = [labels_dict[subfolder]]*len(sub_samples)\n","      samples_labels.extend(sub_labels)\n","      sub_light = [light_domain]*len(sub_samples)\n","      samples_light.extend(sub_light)\n","      sub_file = [file]*len(sub_samples)\n","      samples_filenames.extend(sub_file)\n","      n_subfolder_samples = n_subfolder_samples + len(sub_samples)\n","    database_summary_dict[subfolder] = n_subfolder_samples\n","  print('extracting features')\n","  data = []\n","  for i, sample in enumerate(all_samples):\n","    data_row, features = extract_features(sample)\n","    data.append(data_row)\n","\n","  df = pd.DataFrame(data, columns=features)\n","  df['light'] = samples_light\n","  df['file'] = samples_filenames\n","  df['label'] = samples_labels\n","\n","  print('Done')\n","  return df, features"],"metadata":{"id":"jdQPqKaEdya_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Split"],"metadata":{"id":"XFURuAe4rRag"}},{"cell_type":"code","source":["def split_by_files(df, df0, split_config):\n","  files = np.unique(df['file'].values)\n","  files_ind = [np.where(df['file'].values == f)[0][0] for f in files]\n","  files_labels = df['label'].values[files_ind]\n","  rs = split_config['random_state']\n","  ts = split_config['test_split_files']\n","  files_train, files_test = train_test_split(files, test_size=ts, random_state=rs, stratify=files_labels)\n","  train_df = df[df['file'].isin(files_train)]\n","  test_df = df0[df0['file'].isin(files_test)]\n","  return train_df, test_df"],"metadata":{"id":"fMADz8abAfCm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(labels_dict)\n","print(subf_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04l2VYPKQRUF","executionInfo":{"status":"ok","timestamp":1707207137978,"user_tz":-120,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"e1be9cab-d5f8-4f6c-8c5b-08591edc7846"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}\n","{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}\n"]}]},{"cell_type":"code","source":["subf_dict[3] = 'static-object'"],"metadata":{"id":"yjSqkrE_7P29"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"aRHy7rCLBl73"}},{"cell_type":"code","source":["def evaluate(labels, predictions, caption, plot_cm , print_scores):\n","\n","  classes = np.union1d(labels, predictions)\n","  tick_names = [subf_dict[c] for c in classes]\n","  # Confusion Matrix - Multi class\n","  cm = confusion_matrix(labels, predictions)\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                               display_labels=tick_names)\n","  if plot_cm:\n","    disp.plot()\n","    disp.ax_.set_title(caption)\n","    plt.show()\n","\n","  # Scores #\n","  # f1_s = f1_score(labels, predictions, sample_weight=None, average = 'weighted', zero_division='warn')\n","  report = classification_report(labels, predictions, target_names = tick_names, output_dict=True, digits=3, zero_division = 0)\n","\n","  f1_all = report['weighted avg']['f1-score']\n","\n","  if print_scores == True:\n","      print(classification_report(labels, predictions, target_names = tick_names,digits=3, zero_division = 0))\n","      # print(f1_s)\n","      print('Average F1 score for all classes = ', f1_all)\n","      print('------------------------')\n","      print('UAV report')\n","      print('------------------------')\n","      print('precision = ', report['uav']['precision'])\n","      print('recall = ', report['uav']['recall'])\n","      print('F1 = ', report['uav']['f1-score'])\n","      print('support = ', report['uav']['support'])\n","\n","  return report"],"metadata":{"id":"OcD3qzt_FUIv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experiments"],"metadata":{"id":"ngvs1istUO2a"}},{"cell_type":"code","source":["# record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","# exp_record = pd.DataFrame(columns = record_columns)"],"metadata":{"id":"C-JfJdNhbq-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conf2str(split_config, features_config):\n","  conf_str = 'sd' + str(split_config['sample_duration']) \\\n","  +'of' + str(split_config['overlap_factor']) \\\n","  +'ms' + str(split_config['min_samples']) \\\n","  +'f_date' + str(features_config['date'])\n","  return conf_str"],"metadata":{"id":"rTu7JS3JMywC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_dict = {\n","    'extremum' : np.arange(0,16),\n","    'std' : np.arange(16,20),\n","    'span' : np.arange(20,22),\n","    'med' : np.arange(22,29),\n","    'bbox' : np.arange(29,33),\n","    'corr_sig' : np.arange(33,37),\n","    'full_ratio' : np.arange(37,40),\n","    'curve' : np.arange(40,50),\n","    'pt_ratio' : np.arange(50,61),\n","    'vel_acc_profile' : np.arange(61,79)\n","}"],"metadata":{"id":"XoqR7TgInQyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets(split_config, features_config):\n","  conf_str = conf2str(split_config, features_config)\n","  database_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database_path):\n","    with open(database_path , 'rb') as f:\n","      df, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df, features = prepare_dataset(split_config)\n","    with open(database_path , 'wb') as f:\n","      pickle.dump((df, features), f)\n","\n","  #retrieving 0 overlap matching df\n","  ol0_config = split_config.copy()\n","  ol0_config['overlap_factor'] = 0\n","  conf_str = conf2str(ol0_config, features_config)\n","\n","  database0_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database0_path):\n","    with open(database0_path , 'rb') as f:\n","      df0, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df0, features = prepare_dataset(ol0_config)\n","    with open(database0_path , 'wb') as f:\n","      pickle.dump((df0, features), f)\n","  return df, df0, features"],"metadata":{"id":"AeEmiAjM7akc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","exp_record = pd.DataFrame(columns = record_columns)\n","\n","split_config = {\n","    'sample_duration' : 5,\n","    'overlap_factor' : 0.25,\n","    'subfolders_ind' : [0, 1, 2, 3],\n","    'min_samples' : 10,\n","    'random_state' : 42,\n","    'test_split_files' : 0.2, # 2 for n files\n","    'kfold' : 5\n","  }\n","\n","features_config = {\n","    'date' : 20_1_2024,\n","    'extremum' : True,\n","    'std' : True,\n","    'span' : True,\n","    'med' : True,\n","    'bbox' : True,\n","    'corr_sig' : True,\n","    'full_ratio' : True,\n","    'curve' : True,\n","    'pt_ratio' : True,\n","    'vel_acc_profile' : True\n","  }\n","\n","xgb_config = {\n","    'max_depth' : 6,\n","    'model_random_state' : 42,\n","    'eta' : 0.3\n","  }\n","\n","# split_random_states = [32]\n","# sample_durations = [20]\n","# max_depth = [10]\n","\n","model_random_states = [42]\n","split_random_states = [42]\n","sample_durations = [5, 10, 15, 20, 25]\n","# sample_durations = [10]\n","max_depth = [2, 4, 7, 10]\n","etas = [0.05, 0.1, 1, 3]\n","\n","\n","# max_depth = [4]\n","# etas = [1]\n","\n","model_name = 'XGB'\n","model_config = xgb_config\n","\n","now = datetime.now()\n","date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","#gather data from all datasets and equalize\n","dur_weight_ol25 = []\n","# dur_weight_ol0 = []\n","datasets = []\n","\n","for dur in sample_durations:\n","  split_config['sample_duration'] = dur\n","  datasets.append(get_datasets(split_config, features_config))\n","  dur_weight_ol25.append(len(datasets[-1][0]))\n","  # dur_weight_ol0.append(len(datasets[-1][1]))\n","max_weight_ol25 = max(dur_weight_ol25)\n","# max_weight_ol0 = max(dur_weight_ol0)\n","#empty dataframe\n","df = pd.DataFrame(columns = datasets[0][0].columns)\n","# df0 = pd.DataFrame(columns = datasets[0][0].columns)\n","for i in range(len(dur_weight_ol25)):\n","  #appending full original size - 0 overlap\n","  dataset_dur = datasets[i][0]\n","  df = df.append(dataset_dur)\n","  #appending leftover to balance\n","  leftover = max_weight_ol25 - len(dataset_dur)\n","  df = df.append(dataset_dur.sample(leftover, replace = True))\n","\n","  # #appending full original size - 25 overlap\n","  # dataset_dur = datasets[i][1]\n","  # df0 = df0.append(dataset_dur)\n","  # #appending leftover to balance\n","  # leftover = max_weight_ol0 - len(dataset_dur)\n","  # df0 = df0.append(dataset_dur.sample(leftover, replace = True))\n","\n","_d1, df0, features = get_datasets(split_config, features_config)\n","train_df, test_df = split_by_files(df, df0, split_config) # using only the train df (split should remain constant - getting the same seed as later)\n","# for mrs in model_random_states:\n","#   model_config['model_random_state'] = mrs\n","#   for rs in split_random_states:\n","#     split_config['random_state'] = rs\n","      # for dur in sample_durations:\n","      #   split_config['sample_duration'] = dur\n","for depth in max_depth:\n","  model_config['max_depth'] = depth\n","  for eta in etas:\n","    model_config['eta'] = eta\n","    # print(split_config)\n","    # print(model_config)\n","    #Don't change split_config beyond this point\n","    # _d1, df0, features = get_datasets(split_config, features_config)\n","    # train_df, test_df = split_by_files(df, df0, split_config)\n","\n","    feature_columns = np.empty(0)\n","    for key in features_config.keys():\n","      if features_config[key] == True:\n","        feature_columns = np.hstack([feature_columns, features_dict[key]])\n","\n","    # train_df = train_df_0.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","    train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","    train_df = train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","    train = train_df.iloc[:, feature_columns].values\n","    # test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","    # test = test_df.iloc[:, feature_columns].values\n","    train_labels = train_df['label'].values.astype(np.int64)\n","    train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=train_labels)\n","    # test_labels = test_df['label'].values.astype(np.int64)\n","\n","    #kfold cross validation\n","    # splitting needs to be done according to filenames to not have train and val samples from the same file\n","    files = np.unique(train_df['file'].values)\n","    files_ind = [np.where(train_df['file'].values == f)[0][0] for f in files]\n","    files_labels = train_df['label'].values[files_ind].astype(np.int64)\n","    # use stratified Kfold for imbalance handling\n","    skf = StratifiedKFold(n_splits=split_config['kfold'])\n","    skfold_ind = skf.split(files, files_labels)\n","    uav_precision = []\n","    uav_recall = []\n","    uav_f1 = []\n","    weighted_f1 = []\n","    for (train_ind, val_ind) in skfold_ind:\n","      train_files = files[train_ind]\n","      k_train_df = train_df[train_df['file'].isin(train_files)]\n","      k_train_df = k_train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","      k_train_labels = k_train_df['label'].values.astype(np.int64)\n","      k_train = k_train_df.iloc[:, feature_columns].values\n","      k_train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=k_train_labels)\n","\n","      val_files = files[val_ind]\n","      k_val_df = df0[df0['file'].isin(val_files)] # taking calculations from samples without overlap\n","      k_val_df = k_val_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","      k_val_labels = k_val_df['label'].values.astype(np.int64)\n","      k_val = k_val_df.iloc[:, feature_columns].values\n","\n","      if model_name == 'XGB':\n","        xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=k_train_classes_weights)\n","        xgb_model.fit(k_train, k_train_labels)\n","        k_predictions = xgb_model.predict(k_val)\n","\n","      k_report = evaluate(k_val_labels, k_predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","      uav_precision.append(k_report['uav']['precision'])\n","      uav_recall.append(k_report['uav']['recall'])\n","      uav_f1.append(k_report['uav']['f1-score'])\n","      weighted_f1.append(k_report['weighted avg']['f1-score'])\n","\n","    uav_precision_m = sum(uav_precision)/len(uav_precision)\n","    uav_recall_m = sum(uav_recall)/len(uav_recall)\n","    uav_f1_m = sum(uav_f1)/len(uav_f1)\n","    weighted_f1_m = sum(weighted_f1)/len(weighted_f1)\n","\n","    # print('Fitting all train and test evaluation')\n","    xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=train_classes_weights)\n","    xgb_model.fit(train, train_labels)\n","\n","    for dur in sample_durations:\n","      split_config['sample_duration'] = dur\n","      _d1, df0, features = get_datasets(split_config, features_config)\n","      _d2, test_df = split_by_files(df, df0, split_config)\n","      test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","      test = test_df.iloc[:, feature_columns].values\n","      test_labels = test_df['label'].values.astype(np.int64)\n","      predictions = xgb_model.predict(test)\n","      test_report = evaluate(test_labels, predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","\n","      # wrong_ind = np.where(predictions != test_labels)\n","      # print('wrong on:')\n","      # wrong_files = np.unique(test_df['file'].iloc[wrong_ind])\n","      # print(wrong_files)\n","      ## print(test_df['file'].iloc[wrong_ind])\n","      test_files = np.unique(test_df['file'])\n","\n","      exp_record.loc[len(exp_record)] = [date_time, dur, test_files, split_config.copy(), features_config, model_name, model_config.copy(),\n","                      test_report, uav_precision_m, uav_recall_m, uav_f1_m, weighted_f1_m]"],"metadata":{"id":"1EACVQfwMPdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","# exp_record = pd.DataFrame(columns = record_columns)\n","\n","# split_config = {\n","#     'sample_duration' : 15,\n","#     'overlap_factor' : 0.25,\n","#     'subfolders_ind' : [0, 1, 2, 3],\n","#     'min_samples' : 10,\n","#     'random_state' : 42,\n","#     'test_split_files' : 0.2, # 2 for n files\n","#     'kfold' : 5\n","#   }\n","\n","# features_config = {\n","#     'date' : 20_1_2024,\n","#     'extremum' : True,\n","#     'std' : True,\n","#     'span' : True,\n","#     'med' : True,\n","#     'bbox' : True,\n","#     'corr_sig' : True,\n","#     'full_ratio' : True,\n","#     'curve' : True,\n","#     'pt_ratio' : True,\n","#     'vel_acc_profile' : True\n","#   }\n","\n","# xgb_config = {\n","#     'max_depth' : 6,\n","#     'model_random_state' : 42,\n","#     'eta' : 0.3\n","#   }\n","\n","# # split_random_states = [32]\n","# # sample_durations = [20]\n","# # max_depth = [10]\n","\n","# model_random_states = [42]\n","# split_random_states = [42]\n","# sample_durations = [5, 10, 15, 20, 25]\n","# # sample_durations = [10]\n","# max_depth = [2, 4, 7, 10]\n","# etas = [0.05, 0.1, 1, 3]\n","\n","# # max_depth = [4]\n","# # etas = [1]\n","\n","# model_name = 'XGB'\n","# model_config = xgb_config\n","# now = datetime.now()\n","# date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","# for rs in split_random_states:\n","#   split_config['random_state'] = rs\n","#   for dur in sample_durations:\n","#     split_config['sample_duration'] = dur\n","#     for md in max_depth:\n","#       model_config['max_depth'] = md\n","#       for eta in etas:\n","#         model_config['eta'] = eta\n","#         # print(split_config)\n","#         # print(model_config)\n","#         #Don't change split_config beyond this point\n","#         df, df0, features = get_datasets(split_config, features_config)\n","#         train_df, test_df = split_by_files(df, df0, split_config)\n","#         #patch!!!\n","#         train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","#         test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","\n","#         feature_columns = np.empty(0)\n","#         for key in features_config.keys():\n","#           if features_config[key] == True:\n","#             feature_columns = np.hstack([feature_columns, features_dict[key]])\n","\n","#         # train_df = train_df_0.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","#         # train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","#         train_df = train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","#         train = train_df.iloc[:, feature_columns].values\n","#         # test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","#         test = test_df.iloc[:, feature_columns].values\n","#         train_labels = train_df['label'].values\n","#         test_labels = test_df['label'].values\n","#         train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=train_labels)\n","\n","#         #kfold cross validation\n","#         # splitting needs to be done according to filenames to not have train and val samples from the same file\n","#         files = np.unique(train_df['file'].values)\n","#         files_ind = [np.where(train_df['file'].values == f)[0][0] for f in files]\n","#         files_labels = train_df['label'].values[files_ind]\n","#         # use stratified Kfold for imbalance handling\n","#         skf = StratifiedKFold(n_splits=split_config['kfold'])\n","#         skfold_ind = skf.split(files, files_labels)\n","#         uav_precision = []\n","#         uav_recall = []\n","#         uav_f1 = []\n","#         weighted_f1 = []\n","#         for (train_ind, val_ind) in skfold_ind:\n","#           train_files = files[train_ind]\n","#           k_train_df = train_df[train_df['file'].isin(train_files)]\n","#           k_train_df = k_train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","#           k_train_labels = k_train_df['label'].values\n","#           k_train = k_train_df.iloc[:, feature_columns].values\n","#           k_train_classes_weights = class_weight.compute_sample_weight(class_weight='balanced', y=k_train_labels)\n","\n","#           val_files = files[val_ind]\n","#           k_val_df = df0[df0['file'].isin(val_files)] # taking calculations from samples without overlap\n","#           k_val_df = k_val_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df (note - same rs as the model)\n","#           k_val_labels = k_val_df['label'].values\n","#           k_val = k_val_df.iloc[:, feature_columns].values\n","\n","#           if model_name == 'XGB':\n","#             xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=k_train_classes_weights)\n","#             xgb_model.fit(k_train, k_train_labels)\n","#             k_predictions = xgb_model.predict(k_val)\n","\n","#           k_report = evaluate(k_val_labels, k_predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","#           uav_precision.append(k_report['uav']['precision'])\n","#           uav_recall.append(k_report['uav']['recall'])\n","#           uav_f1.append(k_report['uav']['f1-score'])\n","#           weighted_f1.append(k_report['weighted avg']['f1-score'])\n","\n","#         uav_precision_m = sum(uav_precision)/len(uav_precision)\n","#         uav_recall_m = sum(uav_recall)/len(uav_recall)\n","#         uav_f1_m = sum(uav_f1)/len(uav_f1)\n","#         weighted_f1_m = sum(weighted_f1)/len(weighted_f1)\n","\n","#         # print('Fitting all train and test evaluation')\n","#         if model_name == 'XGB':\n","#           xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", max_depth = model_config['max_depth'], eta = model_config['eta'], seed=42, sample_weight=train_classes_weights)\n","#           xgb_model.fit(train, train_labels)\n","#           predictions = xgb_model.predict(test)\n","#           test_report = evaluate(test_labels, predictions, model_name+'_depth_'+str(model_config['max_depth']), False , False)\n","\n","#         # wrong_ind = np.where(predictions != test_labels)\n","#         # print('wrong on:')\n","#         # print(test_df['file'].iloc[wrong_ind])\n","#         test_files = np.unique(test_df['file'])\n","#         exp_record.loc[len(exp_record)] = [date_time, dur, test_files, split_config.copy(), features_config, model_name, model_config.copy(),\n","#                           test_report, uav_precision_m, uav_recall_m, uav_f1_m, weighted_f1_m]"],"metadata":{"id":"QfM7V3qc6I8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# features"],"metadata":{"id":"yMxHO201ojBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["date_time_ = str(date_time).replace(\"/\",\"_\")\n","results_path = './Features Extraction Results unified durations/features_extraction_results_' + date_time_\n","\n","with open(results_path , 'wb') as f:\n","  pickle.dump(exp_record, f)"],"metadata":{"id":"MRJtp4JPMCVH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exp_record"],"metadata":{"id":"3xPTz6ogGTQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707208053562,"user_tz":-120,"elapsed":8,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"fe6fb1a8-dd81-420a-bbb8-34e597b39c74"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                datetime  Sample Duration  \\\n","0   02/06/2024, 08:14:55                5   \n","1   02/06/2024, 08:14:55               10   \n","2   02/06/2024, 08:14:55               15   \n","3   02/06/2024, 08:14:55               20   \n","4   02/06/2024, 08:14:55               25   \n","..                   ...              ...   \n","75  02/06/2024, 08:14:55                5   \n","76  02/06/2024, 08:14:55               10   \n","77  02/06/2024, 08:14:55               15   \n","78  02/06/2024, 08:14:55               20   \n","79  02/06/2024, 08:14:55               25   \n","\n","                                           Test Files  \\\n","0   [NIR_airplane_20231023-13275_20231023-181630.j...   \n","1   [NIR_airplane_20231023-13275_20231023-181630.j...   \n","2   [NIR_airplane_20231023-13275_20231023-181630.j...   \n","3   [NIR_airplane_20231023-13275_20231023-181630.j...   \n","4   [NIR_airplane_20231023-13275_20231023-181630.j...   \n","..                                                ...   \n","75  [NIR_airplane_20231023-13275_20231023-181630.j...   \n","76  [NIR_airplane_20231023-13275_20231023-181630.j...   \n","77  [NIR_airplane_20231023-13275_20231023-181630.j...   \n","78  [NIR_airplane_20231023-13275_20231023-181630.j...   \n","79  [NIR_airplane_20231023-13275_20231023-181630.j...   \n","\n","                                         Split Config  \\\n","0   {'sample_duration': 5, 'overlap_factor': 0.25,...   \n","1   {'sample_duration': 10, 'overlap_factor': 0.25...   \n","2   {'sample_duration': 15, 'overlap_factor': 0.25...   \n","3   {'sample_duration': 20, 'overlap_factor': 0.25...   \n","4   {'sample_duration': 25, 'overlap_factor': 0.25...   \n","..                                                ...   \n","75  {'sample_duration': 5, 'overlap_factor': 0.25,...   \n","76  {'sample_duration': 10, 'overlap_factor': 0.25...   \n","77  {'sample_duration': 15, 'overlap_factor': 0.25...   \n","78  {'sample_duration': 20, 'overlap_factor': 0.25...   \n","79  {'sample_duration': 25, 'overlap_factor': 0.25...   \n","\n","                                      Features Config Model  \\\n","0   {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","1   {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","2   {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","3   {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","4   {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","..                                                ...   ...   \n","75  {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","76  {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","77  {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","78  {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","79  {'date': 2012024, 'extremum': True, 'std': Tru...   XGB   \n","\n","                                         Model Config  \\\n","0   {'max_depth': 2, 'model_random_state': 42, 'et...   \n","1   {'max_depth': 2, 'model_random_state': 42, 'et...   \n","2   {'max_depth': 2, 'model_random_state': 42, 'et...   \n","3   {'max_depth': 2, 'model_random_state': 42, 'et...   \n","4   {'max_depth': 2, 'model_random_state': 42, 'et...   \n","..                                                ...   \n","75  {'max_depth': 10, 'model_random_state': 42, 'e...   \n","76  {'max_depth': 10, 'model_random_state': 42, 'e...   \n","77  {'max_depth': 10, 'model_random_state': 42, 'e...   \n","78  {'max_depth': 10, 'model_random_state': 42, 'e...   \n","79  {'max_depth': 10, 'model_random_state': 42, 'e...   \n","\n","                                    Evaluation Report  UAV precision  \\\n","0   {'airplane': {'precision': 0.8125, 'recall': 0...       0.852345   \n","1   {'airplane': {'precision': 0.8791208791208791,...       0.852345   \n","2   {'airplane': {'precision': 0.8703703703703703,...       0.852345   \n","3   {'airplane': {'precision': 0.8974358974358975,...       0.852345   \n","4   {'airplane': {'precision': 0.92, 'recall': 0.9...       0.852345   \n","..                                                ...            ...   \n","75  {'airplane': {'precision': 0.5491803278688525,...       0.604430   \n","76  {'airplane': {'precision': 0.49056603773584906...       0.604430   \n","77  {'airplane': {'precision': 0.53125, 'recall': ...       0.604430   \n","78  {'airplane': {'precision': 0.5172413793103449,...       0.604430   \n","79  {'airplane': {'precision': 0.5714285714285714,...       0.604430   \n","\n","    UAV recall    UAV f1  Total f1  \n","0      0.88809  0.865476  0.929535  \n","1      0.88809  0.865476  0.929535  \n","2      0.88809  0.865476  0.929535  \n","3      0.88809  0.865476  0.929535  \n","4      0.88809  0.865476  0.929535  \n","..         ...       ...       ...  \n","75     0.59669  0.561266  0.750652  \n","76     0.59669  0.561266  0.750652  \n","77     0.59669  0.561266  0.750652  \n","78     0.59669  0.561266  0.750652  \n","79     0.59669  0.561266  0.750652  \n","\n","[80 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-f75bcc4d-297e-4230-a651-e1c67159cafc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>datetime</th>\n","      <th>Sample Duration</th>\n","      <th>Test Files</th>\n","      <th>Split Config</th>\n","      <th>Features Config</th>\n","      <th>Model</th>\n","      <th>Model Config</th>\n","      <th>Evaluation Report</th>\n","      <th>UAV precision</th>\n","      <th>UAV recall</th>\n","      <th>UAV f1</th>\n","      <th>Total f1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>5</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 5, 'overlap_factor': 0.25,...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 2, 'model_random_state': 42, 'et...</td>\n","      <td>{'airplane': {'precision': 0.8125, 'recall': 0...</td>\n","      <td>0.852345</td>\n","      <td>0.88809</td>\n","      <td>0.865476</td>\n","      <td>0.929535</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>10</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 10, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 2, 'model_random_state': 42, 'et...</td>\n","      <td>{'airplane': {'precision': 0.8791208791208791,...</td>\n","      <td>0.852345</td>\n","      <td>0.88809</td>\n","      <td>0.865476</td>\n","      <td>0.929535</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>15</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 15, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 2, 'model_random_state': 42, 'et...</td>\n","      <td>{'airplane': {'precision': 0.8703703703703703,...</td>\n","      <td>0.852345</td>\n","      <td>0.88809</td>\n","      <td>0.865476</td>\n","      <td>0.929535</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>20</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 20, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 2, 'model_random_state': 42, 'et...</td>\n","      <td>{'airplane': {'precision': 0.8974358974358975,...</td>\n","      <td>0.852345</td>\n","      <td>0.88809</td>\n","      <td>0.865476</td>\n","      <td>0.929535</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>25</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 25, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 2, 'model_random_state': 42, 'et...</td>\n","      <td>{'airplane': {'precision': 0.92, 'recall': 0.9...</td>\n","      <td>0.852345</td>\n","      <td>0.88809</td>\n","      <td>0.865476</td>\n","      <td>0.929535</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>5</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 5, 'overlap_factor': 0.25,...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 10, 'model_random_state': 42, 'e...</td>\n","      <td>{'airplane': {'precision': 0.5491803278688525,...</td>\n","      <td>0.604430</td>\n","      <td>0.59669</td>\n","      <td>0.561266</td>\n","      <td>0.750652</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>10</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 10, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 10, 'model_random_state': 42, 'e...</td>\n","      <td>{'airplane': {'precision': 0.49056603773584906...</td>\n","      <td>0.604430</td>\n","      <td>0.59669</td>\n","      <td>0.561266</td>\n","      <td>0.750652</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>15</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 15, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 10, 'model_random_state': 42, 'e...</td>\n","      <td>{'airplane': {'precision': 0.53125, 'recall': ...</td>\n","      <td>0.604430</td>\n","      <td>0.59669</td>\n","      <td>0.561266</td>\n","      <td>0.750652</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>20</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 20, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 10, 'model_random_state': 42, 'e...</td>\n","      <td>{'airplane': {'precision': 0.5172413793103449,...</td>\n","      <td>0.604430</td>\n","      <td>0.59669</td>\n","      <td>0.561266</td>\n","      <td>0.750652</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>02/06/2024, 08:14:55</td>\n","      <td>25</td>\n","      <td>[NIR_airplane_20231023-13275_20231023-181630.j...</td>\n","      <td>{'sample_duration': 25, 'overlap_factor': 0.25...</td>\n","      <td>{'date': 2012024, 'extremum': True, 'std': Tru...</td>\n","      <td>XGB</td>\n","      <td>{'max_depth': 10, 'model_random_state': 42, 'e...</td>\n","      <td>{'airplane': {'precision': 0.5714285714285714,...</td>\n","      <td>0.604430</td>\n","      <td>0.59669</td>\n","      <td>0.561266</td>\n","      <td>0.750652</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>80 rows × 12 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f75bcc4d-297e-4230-a651-e1c67159cafc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f75bcc4d-297e-4230-a651-e1c67159cafc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f75bcc4d-297e-4230-a651-e1c67159cafc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-15e3b291-bcf8-46e2-bd43-453256a71318\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15e3b291-bcf8-46e2-bd43-453256a71318')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-15e3b291-bcf8-46e2-bd43-453256a71318 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_63e61543-1c6b-4e03-9315-dbc0285361af\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('exp_record')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_63e61543-1c6b-4e03-9315-dbc0285361af button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('exp_record');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["exp_record.iloc[0]['Evaluation Report']"],"metadata":{"id":"XoN0aN2hjH5Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sample visualization"],"metadata":{"id":"QtjE8uLPBbED"}},{"cell_type":"code","source":["def sample_visualization(sample, start = 0, finish = -999):\n","  \"\"\"\n","  Plots the different preprocessing stages for a single file\n","  Zoom into the plot by specifying the start and end indices\n","  \"\"\"\n","  tt, xx, yy, zz, size_hor, size_ver = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5]\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","\n","  if finish == -999:\n","      start = 0\n","      finish = len(tt)\n","\n","  fig = plt.figure(figsize = [10,10])\n","  ax1 = fig.add_subplot(2, 2, 1, projection='3d')\n","  ax1.plot(xx[start:finish], yy[start:finish], zz[start:finish], '.-b', label='Position')\n","  ax1.set_title('Raw xyz Data')\n","  ax1.set_xlabel('x')\n","  ax1.set_ylabel('y')\n","  ax1.set_zlabel('z')\n","  plt.legend()\n","  #plot cleaned data\n","  cleaned_theta, cleaned_phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","\n","  ax2 = fig.add_subplot(2, 2, 2)\n","  ax2.plot(cleaned_theta[start:finish], cleaned_phi[start:finish], '.-g', label='Angles')\n","  ax2.set_title('Cleaned Angles Data')\n","  ax2.set_xlabel('Azimuth [deg]')\n","  ax2.set_ylabel('Elevation [deg]')\n","  plt.legend()\n","  #plot interpolation\n","  delta = 0.04\n","  new_tt, interp_theta = interpolate_data(tt, cleaned_theta, dt=delta, fixed = False)\n","  new_tt, interp_phi = interpolate_data(tt, cleaned_phi, dt=delta, fixed = False)\n","\n","  #to plot the same span as not interpolated - find the right time marker\n","  new_finish = np.nonzero(new_tt>=tt[finish-1])[0][0]\n","  ax3 = fig.add_subplot(2, 2, 3)\n","  ax3.plot(interp_theta[start:new_finish], interp_phi[start:new_finish], '.-r', label='Interploated angles')\n","  ax3.set_title('Interpolated Data')\n","  ax3.set_xlabel('Azimuth [deg]')\n","  ax3.set_ylabel('Elevation [deg]')\n","  plt.legend()\n","  #plot elevation derivative\n","  # vel_theta = np.diff(interp_theta)/np.diff(new_tt)\n","  vel_phi = np.diff(interp_phi)/np.diff(new_tt)\n","  t_vel = (new_tt[:-1] + new_tt[1:])/2\n","\n","  ax4 = fig.add_subplot(2, 2, 4)\n","  ax4.plot(t_vel[start:new_finish-1], vel_phi[start:new_finish-1], '.-m', label='vel phi')\n","  ax4.set_title('Elevation Derivative')\n","  ax4.set_xlabel('Azimuth [deg]')\n","  ax4.set_ylabel('Elevation [deg]')\n","  plt.legend()"],"metadata":{"id":"JEa3smf2_xb_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1r7p9VanU_m81qruh02RxfDhdsHbCnxHh","timestamp":1707202316280},{"file_id":"1r3QlF2dWJaA7LVwNG0NcHRBC7RAv-28R","timestamp":1704878410179},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"toc_visible":true,"authorship_tag":"ABX9TyPS5R2Zp94YA+VNOFI3RDrZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
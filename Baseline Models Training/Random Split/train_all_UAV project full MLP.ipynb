{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **To Do**\n","- try different data normalization (mean and std?)\n"]},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"q2hR_rZ4hcwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":48410,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"bd99f267-a314-4885-b225-2586a6e8e859"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","from time import time\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","from tqdm import tqdm\n","import copy\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","# from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","\n","# torch\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import Sequential\n","from torch.nn import Sigmoid,ReLU\n","from torch.nn import Linear,Dropout\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.transforms import ToTensor,Compose\n","from torch.optim import SparseAdam,Adam,Adagrad,SGD\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# prompt: Access a folder on google drive and import the data locally\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"EP0cLHeSozdF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"131d88b8-85a1-4971-80d4-dcadee01b7c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/bird/VIS_bird_20230605-406_20230605-142825.json\"\n","# file_path = \"track_data/bird/VIS_bird_20231024-22251_20231024-083633.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231023-6210_20231023-162937.json\"\n","# file_path = \"track_data/airplane/VIS_airplane_20231025-21925_20231025-074948.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10903_20231027-114017.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-11132_20231027-115333.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10582_20231027-113033.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231030-6113_20231030-140702.json\"\n","# file_path = \"track_data/uav/VIS_uav_20231027-10437_20231027-112435.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# file_path = \"track_data/uav/VIS_uav_20230605-108_20230605-130248.json\"\n","# plot_vs_time(file_path, 'vel')"],"metadata":{"id":"TNVFI13GYV1-","executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":4,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Segmentation"],"metadata":{"id":"EmuJtXUpZujJ"}},{"cell_type":"markdown","source":["Assume interpolation of dt = 40 msec.\\\n","For example: a sequence of 5 seconds is equivalent to 25*5 = 125"],"metadata":{"id":"RleKlQDgaQHZ"}},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM","executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":4,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","  sample_duration = samples_config['sample_duration']\n","  overlap_factor = samples_config['overlap_factor']\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","  complete_sample = np.stack([tt, xx, yy, zz, size_hor, size_ver]).T\n","\n","  #segmenting\n","  skip = sample_duration * (1-overlap_factor)\n","  current_index = [0]\n","  end_index = np.nonzero(tt>=tt[0]+sample_duration)[0]\n","\n","  sub_samples = []\n","  while end_index.size:\n","      # print(current_index[0], end_index[0])\n","      if end_index[0] - current_index[0] > samples_config['min_samples']: ### a threshold for a minimum number of datapoints in a sample\n","        sub_samples.append(complete_sample[current_index[0]:end_index[0], :])\n","      end_index = np.nonzero(tt>=tt[current_index[0]]+skip+sample_duration)[0]\n","      current_index = np.nonzero(tt>=tt[current_index[0]]+skip)[0]\n","\n","  return sub_samples, light_domain"],"metadata":{"id":"RL5aYuYSd0v9","executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":4,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX","executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["subf_dict"],"metadata":{"id":"6KbmQvmQbP3U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706336715171,"user_tz":-120,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"2c2d65be-98a1-4b82-b079-15edf5cf0204"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  subfolders = os.listdir(folder)\n","  subfolders_list = [subfolders[i] for i in samples_config['subfolders_ind']]\n","\n","  samples_dict = {}\n","  samples_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","      print(subfolder)\n","      total_samples = 0\n","      subfolder_path = os.path.join(folder, subfolder)\n","      files = os.listdir(subfolder_path)\n","      for file in files:\n","          file_path = os.path.join(subfolder_path, file)\n","          sub_samples, light_domain = segment_file_by_time(file_path, samples_config)\n","          samples_dict[file] = sub_samples\n","          total_samples = total_samples + len(sub_samples)\n","      samples_summary_dict[subfolder] = total_samples\n","  print('samples summary:', samples_summary_dict)\n","  return samples_dict, samples_summary_dict"],"metadata":{"id":"o_AFCYTkh3Dw","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":487,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# samples_config = {\n","#     'sample_duration' : 15,\n","#     'overlap_factor' : 0.25,\n","#     'subfolders_ind' : [0, 1, 2, 3],\n","#     'min_samples' : 10\n","# }\n","# # sample_durations = [25]\n","# sample_durations = [3, 5, 10, 15, 20, 25]\n","# overlaps = [0, 0.25]\n","# for dur in sample_durations:\n","#   print('Sample Duration = ', dur)\n","#   samples_config['sample_duration'] = dur\n","#   for overlap in overlaps:\n","#     print('Overlap = ', overlap)\n","#     samples_config['overlap_factor'] = overlap\n","\n","#     samples_dict, samples_summary_dict = extract_samples(folder, samples_config)\n","#     save_path = './Samples/samples_'+ str(dur) + 'ol' + str(overlap)\n","#     with open(save_path , 'wb') as f:\n","#         pickle.dump((samples_dict, samples_summary_dict), f)"],"metadata":{"id":"Suibu0DoFqWA","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":8,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Helper transformations and feature functions (temp - to be moved to file)"],"metadata":{"id":"cA3lSWL86zUn"}},{"cell_type":"code","source":["def return_span(series):\n","  return series.max() - series.min()"],"metadata":{"id":"zCbytSxp69qn","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def phi_theta_ratio(tt, data_theta, data_phi, pt_ratio_quants, dt=1, time_span = 3):\n","  \"\"\"\n","  \"\"\"\n","  new_tt = np.arange(tt[0], tt[-1]+dt, dt)\n","\n","  interp_data_theta = np.interp(new_tt, tt, data_theta)\n","  pd_data_theta = pd.Series(interp_data_theta)\n","  rolling_data_theta = pd_data_theta.rolling(time_span)\n","  span_theta = rolling_data_theta.apply(return_span)\n","\n","  interp_data_phi = np.interp(new_tt, tt, data_phi)\n","  pd_data_phi = pd.Series(interp_data_phi)\n","  rolling_data_phi = pd_data_phi.rolling(time_span)\n","  span_phi = rolling_data_phi.apply(return_span)\n","  span_ratios = np.arctan(span_phi/span_theta)\n","  span_ratios = span_ratios[~np.isnan(span_ratios)]\n","  span_ratios_quant = np.quantile(span_ratios, pt_ratio_quants)\n","\n","  return span_ratios.min(), span_ratios.max(), span_ratios_quant"],"metadata":{"id":"rr00aVkVePXg","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def PCA_angles_transformation(theta, phi):\n","  \"\"\"returns the data, rotated so that the principle axis of the data in the xy plane is on x\n","  \"\"\"\n","  tp_data = np.stack([theta, phi]).T\n","\n","  pca = PCA(n_components=2, svd_solver='full')\n","  angles_projected = pca.fit_transform(tp_data)\n","\n","  return angles_projected[:,0], angles_projected[:,1]\n","  # new_theta, new_phi = PCA_angles_transformation(cleaned_theta, cleaned_phi)"],"metadata":{"id":"gDei3W0x7_-K","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def angle_span_ratios(theta, phi):\n","  orig_ratio = (phi.max() - phi.min())/(theta.max() - theta.min())\n","  pca_theta, pca_phi = PCA_angles_transformation(theta, phi)\n","  pca_ratio = (pca_phi.max() - pca_phi.min())/(pca_theta.max() - pca_theta.min())\n","  return orig_ratio, pca_ratio"],"metadata":{"id":"fxOjmDRV_Q2r","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def smoothed(data, window = 5):\n","  \"\"\"\n","  Args:\n","    data: A one dimensional array of data.\n","    window: The window of the convolution used for low-pass filtering.\n","\n","  Returns:\n","    A smoothed version of the data\n","  \"\"\"\n","  # Run a low pass filter.\n","  padded_data = np.pad(data, (window//2, window//2), 'edge')\n","  smooth_data = np.convolve(padded_data, np.ones(window) / window, mode='valid')\n","  return smooth_data"],"metadata":{"id":"9PA7CITmDlvt","executionInfo":{"status":"ok","timestamp":1706336715656,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Extract features"],"metadata":{"id":"73loRkCF7vye"}},{"cell_type":"code","source":["  t1 = np.arange(2, 5, 0.04)\n","  t2 = np.arange(5, 6, 0.04)\n","  part1 = (0.5/5**2)*t1**2 + 0.5\n","  part2 = 1 - 0.5*(t2-5)\n","  filt1 = np.flip(np.hstack([part1, part2]))\n","  t4 = np.arange(-5, -2, 0.04)\n","  t3 = np.arange(-6, -5, 0.04)\n","  part3 = -1 - 0.5*(t3+5)\n","  part4 = -(0.5/5**2)*t4**2 - 0.5\n","  filt2 = np.flip(np.hstack([part3, part4]))"],"metadata":{"id":"IoV6bPfHbZ6g","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def extract_features(sample):\n","  tt, xx, yy, zz, size_hor, size_ver = sample[:, 0], sample[:, 1], sample[:, 2], sample[:, 3], sample[:, 4], sample[:, 5]\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  # Basic transformations\n","  #standerdizing to start from 0\n","  phi = phi - phi[0]\n","  theta = theta - theta[0]\n","  # clean\n","  # xx, yy, zz = clean_3D_data_w_split(tt, xx, yy, zz, factor = 3, window = 5, threshold = -999)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999, replace_by = 'med')\n","\n","  # interpolate\n","  delta = 0.04\n","  interp_tt, interp_theta = interpolate_data(tt, theta, dt=delta, fixed = False)\n","  interp_tt, interp_phi = interpolate_data(tt, phi, dt=delta, fixed = False)\n","  # interp_tt, interp_xx = interpolate_data(tt, xx, dt=delta, fixed = False)\n","  # interp_tt, interp_yy = interpolate_data(tt, yy, dt=delta, fixed = False)\n","  # interp_tt, interp_zz = interpolate_data(tt, zz, dt=delta, fixed = False)\n","  interp_tt, interp_size_hor = interpolate_data(tt, size_hor, dt=delta, fixed = False)\n","  interp_tt, interp_size_ver = interpolate_data(tt, size_ver, dt=delta, fixed = False)\n","  fix_interp_tt, fix_interp_theta = interpolate_data(tt, theta, dt=delta, fixed = True)\n","  fix_interp_tt, fix_interp_phi = interpolate_data(tt, phi, dt=delta, fixed = True)\n","\n","  # derive\n","  interp_vel_theta, interp_vel_phi, interp_t_vel = derive(interp_theta, interp_phi, interp_tt)\n","  vel_theta, vel_phi, t_vel = derive(theta, phi, tt)\n","  abs_vel = np.sqrt(vel_theta**2 + vel_phi**2)\n","  acc_theta, acc_phi, t_acc = derive(vel_theta, vel_phi, t_vel)\n","  abs_acc = np.sqrt(acc_theta**2 + acc_phi**2)\n","  fix_vel_theta, fix_vel_phi, t_vel = derive(fix_interp_theta, fix_interp_phi, fix_interp_tt)\n","\n","  # rolling window smoothing\n","  s_theta = smoothed(theta, window = 5)\n","  s_phi = smoothed(phi, window = 5)\n","  delta = 0.04\n","  # interp_tt, s_interp_theta = interpolate_data(tt, s_theta, dt=delta, fixed = False)\n","  # interp_tt, s_interp_phi = interpolate_data(tt, s_phi, dt=delta, fixed = False)\n","  # s_vel_theta = np.diff(s_interp_theta)/np.diff(interp_tt)\n","  # s_vel_phi = np.diff(s_interp_phi)/np.diff(interp_tt)\n","  # s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_vel_theta, s_vel_phi, s_t_vel = derive(s_theta, s_phi, tt)\n","  s_abs_vel = np.sqrt(s_vel_theta**2 + s_vel_phi**2)\n","  s_acc_theta, s_acc_phi, s_t_acc = derive(s_vel_theta, s_vel_phi, s_t_vel)\n","  s_abs_acc = np.sqrt(s_acc_theta**2 + s_acc_phi**2)\n","\n","  # FEATURES\n","  returning_features = []\n","  feature_names = []\n","  # extreme values (scale dependent)\n","  max_size_hor = size_hor.max()\n","  max_size_ver = size_ver.max()\n","  max_elevation = phi.max()\n","  min_elevation = phi.min()\n","  max_theta_vel =  np.abs(vel_theta).max()\n","  max_phi_vel =  np.abs(vel_phi).max()\n","  max_vel = abs_vel.max()\n","  max_theta_acc =  np.abs(acc_theta).max()\n","  max_phi_acc =  np.abs(acc_phi).max()\n","  max_acc = abs_acc.max()\n","  returning_features.extend([max_elevation, min_elevation, max_theta_vel, max_phi_vel, max_vel, max_theta_acc, max_phi_acc, max_acc])\n","  feature_names.extend(['max_elevation', 'min_elevation', 'max_theta_vel', 'max_phi_vel', 'max_vel', 'max_theta_acc', 'max_phi_acc', 'max_acc'])\n","\n","  s_max_elevation = s_phi.max()\n","  s_min_elevation = s_phi.min()\n","  s_max_theta_vel = np.abs(s_vel_theta).max()\n","  s_max_phi_vel = np.abs(s_vel_phi).max()\n","  s_max_vel = s_abs_vel.max()\n","  s_med_vel = np.median(s_abs_vel)\n","  s_max_theta_acc = np.abs(s_acc_theta).max()\n","  s_max_phi_acc = np.abs(s_acc_phi).max()\n","  s_max_acc = s_abs_acc.max()\n","  s_med_acc = np.median(s_abs_acc)\n","  returning_features.extend([s_max_elevation, s_min_elevation, s_max_theta_vel, s_max_phi_vel, s_max_vel, s_max_theta_acc, s_max_phi_acc, s_max_acc])\n","  feature_names.extend(['s_max_elevation', 's_min_elevation', 's_max_theta_vel', 's_max_phi_vel', 's_max_vel', 's_max_theta_acc', 's_max_phi_acc', 's_max_acc'])\n","\n","  # statistics and noise\n","  theta_std = local_std(interp_theta, window = 10)\n","  phi_std = local_std(interp_phi, window = 10)\n","  theta_vel_std = local_std(interp_vel_theta, window = 10)\n","  phi_vel_std = local_std(interp_vel_phi, window = 10)\n","  returning_features.extend([theta_std, phi_std, theta_vel_std, phi_vel_std])\n","  feature_names.extend(['theta_std', 'phi_std', 'theta_vel_std', 'phi_vel_std'])\n","\n","  # span\n","  theta_span = theta.max() - theta.min()\n","  phi_span = phi.max() - phi.min()\n","  returning_features.extend([theta_span, phi_span])\n","  feature_names.extend(['theta_span', 'phi_span'])\n","\n","  # avg values\n","  med_elevation = np.median(phi)\n","  med_size_hor = np.median(size_hor)\n","  med_size_ver = np.median(size_ver)\n","  med_theta_vel =  np.median(vel_theta)\n","  med_phi_vel =  np.median(vel_phi)\n","  med_theta_acc =  np.median(acc_theta)\n","  med_phi_acc =  np.median(acc_phi)\n","  returning_features.extend([med_elevation, med_theta_vel, med_phi_vel, med_theta_acc, med_phi_acc, s_med_vel, s_med_acc])\n","  feature_names.extend(['med_elevation', 'med_theta_vel', 'med_phi_vel', 'med_theta_acc', 'med_phi_acc', 's_med_vel', 's_med_acc'])\n","\n","  # Bounding box data\n","  returning_features.extend([max_size_hor, max_size_ver, med_size_hor, med_size_ver])\n","  feature_names.extend(['max_size_hor', 'max_size_ver', 'med_size_hor', 'med_size_ver'])\n","\n","\n","  # correlations\n","  # print(np.any(np.isinf(vel_theta)), np.any(np.isinf(vel_phi)))\n","  ascending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.max(), filt1, mode='valid'))\n","  decending_sig = np.max(np.convolve(fix_vel_phi/fix_vel_phi.min(), filt2, mode='valid'))\n","\n","  pt_vel_corr, _ = pearsonr(np.abs(vel_theta), np.abs(vel_phi))\n","  pt_acc_corr, _ = pearsonr(np.abs(acc_theta), np.abs(acc_phi))\n","\n","  returning_features.extend([ascending_sig, decending_sig, pt_vel_corr, pt_acc_corr])\n","  feature_names.extend(['ascending_sig', 'decending_sig', 'pt_vel_corr', 'pt_acc_corr'])\n","\n","  # FFT\n","\n","  # yf = fft(fix_vel_phi)\n","  # yf_trimmed_abs = np.abs(yf[0:len(yf)//2+1])\n","\n","  # ratios\n","  orig_ratio, pca_ratio = angle_span_ratios(theta, phi)\n","  std_vs_span = np.sqrt((theta_std**2 + phi_std**2)/(theta_span**2 + phi_span**2))\n","  returning_features.extend([orig_ratio, pca_ratio, std_vs_span])\n","  feature_names.extend(['orig_ratio', 'pca_ratio', 'std_vs_span'])\n","\n","  # scale independent features\n","  # measuring geometric curvature\n","\n","  discrete_angle_dist = np.sqrt(np.diff(interp_theta)**2 + np.diff(interp_phi)**2)\n","  cumsum_angles_dist = np.cumsum(discrete_angle_dist)\n","  path_total_length = cumsum_angles_dist[-1]\n","  cum_dist = np.hstack([0, cumsum_angles_dist])\n","\n","  angles = np.stack([interp_theta, interp_phi]).T\n","  eucl_dist = cdist(angles, angles, 'euclidean') + np.ones([len(interp_theta), len(interp_theta)])*1e-6 #added to avoid devision by 0\n","\n","  path_dist = np.abs(cum_dist[:, None] - cum_dist)\n","  dist_ratio = path_dist/eucl_dist\n","  distance_buffer = 5 #removing ratios of points close to one another, since this may be noisy\n","  all_curve_ratios = sum((dist_ratio[i,i+distance_buffer:].tolist() for i in range(dist_ratio.shape[0])), [])\n","  curve_quants = np.arange(0.1, 0.95, 0.1)\n","  curve_quantiles = np.quantile(all_curve_ratios, curve_quants)\n","  max_curve_ratio = np.max(all_curve_ratios)\n","  returning_features.extend([*curve_quantiles, max_curve_ratio])\n","  q_names = ['curve quant ' + str(round(q, 2)) for q in curve_quants]\n","  feature_names.extend(q_names + ['max_curve_ratio'])\n","\n","  # measuring angles ratios (quantiles):\n","  # e.g. object rising fast without changing azimuth, or changing azimuth without elevation\n","  pt_ratio_quants = np.arange(0.1, 0.95, 0.1)\n","  min_span_ratios, max_span_ratios, span_ratios_quant = phi_theta_ratio(tt, theta, phi, pt_ratio_quants, dt=1, time_span = 3)\n","  returning_features.extend([*span_ratios_quant, max_span_ratios, min_span_ratios])\n","  q_names = ['pt ratio quant ' + str(round(q, 2)) for q in pt_ratio_quants]\n","  feature_names.extend(q_names + ['max_span_ratios', 'min_span_ratios'])\n","\n","  # velocity and acceleration profiles (ratios)\n","  motion_quants = np.arange(0.1, 0.95, 0.1)\n","  vel_quantiles = np.quantile(s_abs_vel, motion_quants)\n","  vel_profile = vel_quantiles/s_med_vel\n","  acc_quantiles = np.quantile(s_abs_acc, motion_quants)\n","  acc_profile = acc_quantiles/s_med_acc\n","  q_vel_names = ['vel quant ' + str(round(q, 2)) for q in motion_quants]\n","  q_acc_names = ['acc quant ' + str(round(q, 2)) for q in motion_quants]\n","  returning_features.extend([*vel_profile, *acc_profile])\n","  feature_names.extend(q_vel_names + q_acc_names)\n","\n","  return returning_features, feature_names"],"metadata":{"id":"yEl_k3nM3aEf","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def prepare_dataset(split_config):\n","  print('collecting samples')\n","  # subfolders_list = [subf_dict[i] for i in split_config['subfolders_ind']]\n","  subfolders_list = [subf_dict[i] for i in np.arange(4)]\n","  save_path = './Samples/samples_'+ str(split_config['sample_duration']) + 'ol' + str(split_config['overlap_factor'])\n","  with open(save_path , 'rb') as f:\n","    samples_dict, samples_summary_dict = pickle.load(f)\n","\n","  # files_labels = []\n","  all_samples = []\n","  samples_labels = []\n","  samples_filenames = []\n","  samples_light = []\n","  database_summary_dict = {}\n","\n","  for subfolder in subfolders_list:\n","    subfolder_path = os.path.join(folder, subfolder)\n","    files = os.listdir(subfolder_path) # Can also be taken from samples_dict keys\n","    # files_labels.extend([labels_dict[subfolder]]*len(files))\n","    n_subfolder_samples = 0\n","    for file in files:\n","      light_domain = file[:3]\n","      sub_samples = samples_dict[file]\n","      all_samples.extend(sub_samples)\n","      sub_labels = [labels_dict[subfolder]]*len(sub_samples)\n","      samples_labels.extend(sub_labels)\n","      sub_light = [light_domain]*len(sub_samples)\n","      samples_light.extend(sub_light)\n","      sub_file = [file]*len(sub_samples)\n","      samples_filenames.extend(sub_file)\n","      n_subfolder_samples = n_subfolder_samples + len(sub_samples)\n","    database_summary_dict[subfolder] = n_subfolder_samples\n","  print('extracting features')\n","  data = []\n","  for i, sample in enumerate(all_samples):\n","    data_row, features = extract_features(sample)\n","    data.append(data_row)\n","\n","  df = pd.DataFrame(data, columns=features)\n","  df['light'] = samples_light\n","  df['file'] = samples_filenames\n","  df['label'] = samples_labels\n","\n","  print('Done')\n","  return df, features"],"metadata":{"id":"jdQPqKaEdya_","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["### Split"],"metadata":{"id":"XFURuAe4rRag"}},{"cell_type":"code","source":["def split_by_files(df, df0, split_config):\n","  files = np.unique(df['file'].values)\n","  files_ind = [np.where(df['file'].values == f)[0][0] for f in files]\n","  files_labels = df['label'].values[files_ind]\n","  rs = split_config['random_state']\n","  ts = split_config['test_split_files']\n","  files_train, files_test = train_test_split(files, test_size=ts, random_state=rs, stratify=files_labels)\n","  train_df = df[df['file'].isin(files_train)]\n","  test_df = df0[df0['file'].isin(files_test)]\n","  return train_df, test_df"],"metadata":{"id":"fMADz8abAfCm","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":7,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print(labels_dict)\n","print(subf_dict)"],"metadata":{"id":"04l2VYPKQRUF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"51effe2d-4c49-474c-fd47-292eebee49c1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}\n","{0: 'airplane', 1: 'uav', 2: 'bird', 3: 'static-object'}\n"]}]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"aRHy7rCLBl73"}},{"cell_type":"code","source":["def evaluate(labels, predictions, caption, plot_cm , print_scores):\n","\n","  classes = np.union1d(labels, predictions)\n","  tick_names = [subf_dict[c] for c in classes]\n","  # Confusion Matrix - Multi class\n","  cm = confusion_matrix(labels, predictions)\n","  disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                               display_labels=tick_names)\n","  if plot_cm:\n","    disp.plot()\n","    disp.ax_.set_title(caption)\n","    plt.show()\n","\n","  # Scores #\n","  # f1_s = f1_score(labels, predictions, sample_weight=None, average = 'weighted', zero_division='warn')\n","  report = classification_report(labels, predictions, target_names = tick_names, output_dict=True, digits=3, zero_division = 0)\n","\n","  f1_all = report['weighted avg']['f1-score']\n","\n","  if print_scores == True:\n","      print(classification_report(labels, predictions, target_names = tick_names,digits=3, zero_division = 0))\n","      # print(f1_s)\n","      print('Average F1 score for all classes = ', f1_all)\n","      print('------------------------')\n","      print('UAV report')\n","      print('------------------------')\n","      print('precision = ', report['uav']['precision'])\n","      print('recall = ', report['uav']['recall'])\n","      print('F1 = ', report['uav']['f1-score'])\n","      print('support = ', report['uav']['support'])\n","\n","  return report"],"metadata":{"id":"OcD3qzt_FUIv","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"AJcGVXtbWEYN"}},{"cell_type":"code","source":["class MLP(torch.nn.Module):\n","    def __init__(self, config):\n","        super(MLP, self).__init__()\n","\n","        self.do = config['dropout']\n","        self.num_classes = config['num_classes']\n","        self.num_features = config['num_features']\n","\n","        self.input_layer = torch.nn.Linear(in_features=self.num_features, out_features = config['layers'][0])\n","        self.fc_layers = torch.nn.ModuleList()\n","        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n","            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n","\n","        self.dropout = torch.nn.Dropout(self.do)\n","        self.logits = torch.nn.Linear(in_features=config['layers'][-1] , out_features=self.num_classes)\n","\n","    def forward(self, samples):\n","        samples = self.dropout(samples)\n","        mlp_vector = self.input_layer(samples)\n","        mlp_vector = torch.nn.ReLU()(mlp_vector)\n","        # torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n","\n","        for idx, _ in enumerate(range(len(self.fc_layers))):\n","            mlp_vector = self.dropout(mlp_vector)\n","            mlp_vector = self.fc_layers[idx](mlp_vector)\n","            mlp_vector = torch.nn.ReLU()(mlp_vector)\n","\n","        mlp_vector = self.dropout(mlp_vector)\n","        logits = self.logits(mlp_vector)\n","        # output = self.sigmoid(logits).squeeze()\n","        return logits"],"metadata":{"id":"q_fyXITbWDkx","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def initialize_weights(model):\n","  if isinstance(model, nn.Linear):\n","    nn.init.xavier_uniform_(model.weight, gain=1)\n","    nn.init.constant_(model.bias, 0)"],"metadata":{"id":"5vTZtyytHhyJ","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Training class"],"metadata":{"id":"Hkp3Svx1HQIX"}},{"cell_type":"code","source":["class Training(object):\n","\n","  def __init__(self, model, config):\n","    self.config = config\n","    self.model = model.to(self.config['device'])\n","    self.optimizer = config['optimizer_type'](model.parameters(), **config['optimizer_parameter'])\n","    self.criterion = config['criterion'](weight = config['class_weights']).to(self.config['device'])\n","    self.dl_train = dl_train\n","\n","  def train(self):\n","    self.train_loss_history = []\n","    self.eval_loss_history = []\n","    self.eval_uav_precision_history = []\n","    self.eval_uav_recall_history = []\n","    self.eval_uav_f1_history = []\n","    self.eval_total_f1_history = []\n","\n","    epochs_without_improvement = 0\n","    best_f1 = None\n","\n","    train_start = time()\n","\n","    for epoch in range(self.config['n_epochs']):\n","    # for epoch in range(1):\n","      self.train_epoch()\n","      self.evaluate_epoch(dl_val)\n","      if self.config['verbose']:\n","        print(f'epoch {epoch}: loss = {self.train_loss_history[-1]}, f1 = {self.eval_uav_f1_history[-1]}')\n","      #check for early stopping\n","      if not best_f1 or self.eval_uav_f1_history[-1] > best_f1:\n","        best_f1 = self.eval_uav_f1_history[-1]\n","        # print(\"best uav f1 \", best_f1)\n","        best_precision = self.eval_uav_precision_history[-1]\n","        best_recall = self.eval_uav_recall_history[-1]\n","        best_loss = self.eval_loss_history[-1]\n","        best_total_f1 = self.eval_total_f1_history[-1]\n","        epochs_without_improvement = 0\n","        #print (\"Achieved lower validation loss, save model at epoch number {} \".format(epoch + 1) )\n","        best_model = copy.deepcopy(self.model.state_dict())\n","      else:\n","        epochs_without_improvement += 1\n","\n","      if epochs_without_improvement == self.config['early_stopping']:\n","        if self.config['verbose']:\n","            print('\\nEarly stoping after {} epochs. validation f1 did not imporve for more than {} epcochs'.format(epoch, self.config['early_stopping']))\n","        break\n","    self.training_time = time() - train_start\n","\n","    # load best model and best performance\n","    self.model.load_state_dict(best_model)\n","    if self.config['verbose']:\n","        print('\\nFinished Training:')\n","        print('Best metrics are:')\n","        # print('Evaluation LogLoss = '.format(best_loss))\n","\n","        print(f'Best f1 eval = {best_f1}')\n","        print(f'Best recall eval= {best_recall}')\n","        print(f'Best precision eval= {best_precision}')\n","        print(f'Best total f1 eval= {best_total_f1}')\n","\n","  def train_epoch(self):\n","    self.epoch_train_loss   = 0\n","    self.model.train() # train mode\n","    # ii = 0\n","    for batch in tqdm(self.dl_train, disable=(not self.config['verbose'])):\n","      # ii +=1\n","      self.train_batch(batch)\n","      # if ii>1:\n","      #   break\n","    self.train_loss_history.append(self.epoch_train_loss/len(self.dl_train))\n","\n","  def train_batch(self, batch):\n","    samples, labels = batch\n","    # Send tensors to GPU\n","    samples = samples.to(device)\n","    labels = labels.to(device)\n","\n","    # pred = self.model(user_indices, item_indices).squeeze().type(torch.DoubleTensor)\n","    pred = self.model(samples)#.type(torch.float64)#.type(torch.DoubleTensor)\n","    # print(pred.get_device())\n","    # print('pred = ', pred)\n","\n","    # loss = self.criterion(pred, labels.float()) # can make label float in creation!!!!!!!!!!!!!!!!!!!!\n","    # labels = labels[:, None]\n","    # print('labels = ', labels)\n","    loss = self.criterion(pred, labels)\n","    # loss = self.criterion(pred, labels)\n","\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","    self.epoch_train_loss += loss.item()\n","\n","  def evaluate_epoch(self, dl_eval):\n","    self.eval_labels = []\n","    self.eval_predictions = []\n","    self.epoch_eval_loss = 0\n","\n","    self.model.eval() #evaluation mode\n","    with torch.no_grad():\n","      for batch in tqdm(dl_eval, disable=(not self.config['verbose'])):\n","      # for batch in tqdm(dl_val, disable=(not self.config['verbose'])):\n","      # for batch in tqdm(dl_test, disable=(not self.config['verbose'])):\n","        self.eval_batch(batch)\n","    # print(self.eval_labels)\n","    # print(self.eval_predictions)\n","    # print(type(self.eval_labels))\n","    # print(type(self.eval_predictions))\n","    # print(len(self.eval_labels))\n","    # print(len(self.eval_predictions))\n","\n","    # print('self.eval_labels ', self.eval_labels)\n","    # print('self.eval_predictions ', self.eval_predictions)\n","    classes = np.union1d(self.eval_labels, self.eval_predictions)\n","    tick_names = [subf_dict[c] for c in classes]\n","    report = classification_report(self.eval_labels, self.eval_predictions, target_names = tick_names, output_dict=True, digits=3, zero_division = 0)\n","    # print(report)\n","    self.report = report\n","    self.eval_loss_history.append(self.epoch_eval_loss/len(dl_val))\n","    self.eval_uav_precision_history.append(report['uav']['precision'])\n","    self.eval_uav_recall_history.append(report['uav']['recall'])\n","    self.eval_uav_f1_history.append(report['uav']['f1-score'])\n","    self.eval_total_f1_history.append(report['weighted avg']['f1-score'])\n","\n","  def eval_batch(self, batch):\n","    samples, labels = batch\n","    # Send tensors to GPU\n","    samples = samples.to(self.config['device'])\n","    labels = labels.to(self.config['device'])\n","\n","    pred = self.model(samples).detach()\n","    eval_loss = self.criterion(pred, labels)\n","    # pred = self.model(samples).detach().cpu()\n","    # print(labels)\n","    # print(torch.argmax(pred, dim = 1))\n","    pred = pred.cpu()\n","    labels = labels.cpu()\n","    self.eval_predictions.extend(torch.argmax(pred, dim = 1))\n","    self.eval_labels.extend(labels)\n","\n","    # eval_loss = self.criterion(pred.type(torch.float64), labels)\n","    self.epoch_eval_loss += eval_loss.item()"],"metadata":{"id":"g-P7oWmYoeZt","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def plot_training_summary(training_model):\n","  epochs = np.arange(len(training_model.train_loss_history)) + 1\n","  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","  ax1.plot(epochs, training_model.train_loss_history, 'r', label = 'train loss')\n","  ax1.plot(epochs, training_model.eval_loss_history, 'orange', label = 'eval loss')\n","  ax1.set_xlabel('Epochs')\n","  ax1.set_ylabel('Loss')\n","  ax1.legend()\n","  ax2.plot(epochs, training_model.eval_uav_precision_history, 'r', label = 'uav precision')\n","  ax2.plot(epochs, training_model.eval_uav_recall_history, 'g', label = 'uav recall')\n","  ax2.plot(epochs, training_model.eval_uav_f1_history, 'b', label = 'uav f1')\n","  ax2.plot(epochs, training_model.eval_total_f1_history, 'm', label = 'total f1')\n","  ax2.set_xlabel('Epochs')\n","  ax2.set_ylabel('Metrics')\n","  ax2.legend()"],"metadata":{"id":"cWjc24pvHlIH","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["## Experiments"],"metadata":{"id":"ngvs1istUO2a"}},{"cell_type":"code","source":["# record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","# exp_record = pd.DataFrame(columns = record_columns)"],"metadata":{"id":"C-JfJdNhbq-N","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def conf2str(split_config, features_config):\n","  conf_str = 'sd' + str(split_config['sample_duration']) \\\n","  +'of' + str(split_config['overlap_factor']) \\\n","  +'ms' + str(split_config['min_samples']) \\\n","  +'f_date' + str(features_config['date'])\n","  return conf_str"],"metadata":{"id":"rTu7JS3JMywC","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["features_dict = {\n","    'extremum' : np.arange(0,16),\n","    'std' : np.arange(16,20),\n","    'span' : np.arange(20,22),\n","    'med' : np.arange(22,29),\n","    'bbox' : np.arange(29,33),\n","    'corr_sig' : np.arange(33,37),\n","    'full_ratio' : np.arange(37,40),\n","    'curve' : np.arange(40,50),\n","    'pt_ratio' : np.arange(50,61),\n","    'vel_acc_profile' : np.arange(61,79)\n","}"],"metadata":{"id":"XoqR7TgInQyg","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":4,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def get_datasets(split_config, features_config):\n","  conf_str = conf2str(split_config, features_config)\n","  database_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database_path):\n","    with open(database_path , 'rb') as f:\n","      df, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df, features = prepare_dataset(split_config)\n","    with open(database_path , 'wb') as f:\n","      pickle.dump((df, features), f)\n","\n","  #retrieving 0 overlap matching df\n","  ol0_config = split_config.copy()\n","  ol0_config['overlap_factor'] = 0\n","  conf_str = conf2str(ol0_config, features_config)\n","\n","  database0_path = './Features Databases/database_'+ conf_str\n","  if os.path.isfile(database0_path):\n","    with open(database0_path , 'rb') as f:\n","      df0, features = pickle.load(f)\n","      # print('Loading dataset')\n","  else:\n","    df0, features = prepare_dataset(ol0_config)\n","    with open(database0_path , 'wb') as f:\n","      pickle.dump((df0, features), f)\n","  return df, df0, features"],"metadata":{"id":"AeEmiAjM7akc","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def scale_dataset(df, df0, features):\n","  # scaling by df\n","  for feature in features:\n","    if df[feature].std() > 0:\n","      col_min = df[feature].min()\n","      col_max = df[feature].max()\n","      df[feature] = (df[feature] - col_min) / (col_max - col_min)\n","      df0[feature] = (df0[feature] - col_min) / (col_max - col_min)\n","      # df[feature] = (df[feature]-df[feature].mean())/df[feature].std()\n","      # df0[feature] = (df0[feature]-df0[feature].mean())/df0[feature].std()\n","  return df, df0"],"metadata":{"id":"5P_yos1Z-Ld2","executionInfo":{"status":"ok","timestamp":1706336715657,"user_tz":-120,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["record_columns = ['datetime', 'Sample Duration', 'Test Files', 'Split Config', 'Features Config', 'Model', 'Model Config', 'Training Config', 'Evaluation Report', 'UAV precision', 'UAV recall', 'UAV f1', 'Total f1']\n","exp_record = pd.DataFrame(columns = record_columns)\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","split_config = {\n","    'sample_duration' : 15,\n","    'overlap_factor' : 0.25,\n","    'subfolders_ind' : [0, 1, 2, 3],\n","    'min_samples' : 10,\n","    'random_state' : 24,\n","    'test_split_files' : 0.2, # 2 for n files\n","    'kfold' : 5\n","  }\n","\n","features_config = {\n","    'date' : 20_1_2024,\n","    'extremum' : True,\n","    'std' : True,\n","    'span' : True,\n","    'med' : True,\n","    'bbox' : True,\n","    'corr_sig' : True,\n","    'full_ratio' : True,\n","    'curve' : True,\n","    'pt_ratio' : True,\n","    'vel_acc_profile' : True\n","  }\n","\n","\n","# mlp_config = {'num_classes' : len(split_config['subfolders_ind']),'layers': [128,64,32,16], 'dropout': 0, 'model_random_state' : 42}\n","mlp_config = {'num_classes' : len(split_config['subfolders_ind']),'layers': [256, 128, 64, 32, 16], 'dropout': 0, 'model_random_state' : 42}\n","training_config = {'batch_size': 16, 'optimizer_type': Adam, 'optimizer_parameter': {'lr': 0.0001}, 'class_weights' : torch.tensor([0.1, 0.5, 0.25, 0.15]), \\\n","                          'criterion' : nn.CrossEntropyLoss, 'n_epochs' : 250, 'early_stopping' : 80, 'verbose' : True, 'device' : device}\n","\n","# split_random_states = [32]\n","# sample_durations = [20]\n","model_random_states = [42]\n","split_random_states = [42]\n","sample_durations = [10]\n","# sample_durations = [10]\n","layerss = [[256, 128, 64, 32, 16]] #c1\n","# layerss = [[256, 256, 128, 32]] #c3\n","# layerss = [[80, 80]] #c2\n","# dropouts = [0, 0.25, 0.5]\n","dropouts = [0]\n","# lrs = [0.00005, 0.0001, 0.0003, 0.001, 0.005]\n","# batch_sizes = [16, 32, 64]\n","lrs = [0.001]\n","batch_sizes = [32]\n","# class_weightss = [torch.tensor(float('nan')), torch.tensor([0.15, 0.5, 0.25, 0.1])]\n","class_weightss = [torch.tensor([0.15, 0.5, 0.25, 0.1])]\n","\n","model_name = 'MLP'\n","model_config = mlp_config\n","now = datetime.now()\n","date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","for mrs in model_random_states:\n","  model_config['model_random_state'] = mrs\n","  for rs in split_random_states:\n","    split_config['random_state'] = rs\n","    for dur in sample_durations:\n","      split_config['sample_duration'] = dur\n","      for layers in layerss:\n","        model_config['layers'] = layers\n","        for do in dropouts:\n","          model_config['dropout'] = do\n","          for lr in lrs:\n","            training_config['optimizer_parameter']['lr'] = lr\n","            for batch_size in batch_sizes:\n","              training_config['batch_size'] = batch_size\n","              for cw in class_weightss:\n","                training_config['class_weights'] = cw\n","                # print(split_config)\n","                # print(model_config)\n","                #Don't change split_config beyond this point\n","                df, df0, features = get_datasets(split_config, features_config)\n","                # df.fillna(0, inplace=True)\n","                # df0.fillna(0, inplace=True)\n","                feature_columns = np.empty(0)\n","                for key in features_config.keys():\n","                  if features_config[key] == True:\n","                    feature_columns = np.hstack([feature_columns, features_dict[key]])\n","\n","                df, df0 = scale_dataset(df, df0, features)\n","                train_df, test_df = split_by_files(df, df0, split_config)\n","                test_df = test_df[test_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","                test = torch.tensor(test_df.iloc[:, feature_columns].values, dtype = torch.float32)\n","                test_labels = torch.tensor(test_df['label'].values)\n","                test_data = list(zip(test, test_labels))\n","                dl_test = DataLoader(test_data, batch_size = training_config['batch_size'])\n","\n","\n","                # train_df, val_df = split_by_files(train_df, df0, split_config)\n","                train_df = train_df[train_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","                train_df = train_df.sample(frac = 1, random_state = model_config['model_random_state']) #shuffled df\n","                train = torch.tensor(train_df.iloc[:, feature_columns].values, dtype = torch.float32)\n","                # val_df = val_df[val_df['label'].isin(split_config['subfolders_ind'])] # removing the folders we don't want to analyze (in split_config)\n","                # val = torch.tensor(val_df.iloc[:, feature_columns].values, dtype = torch.float32)\n","                train_labels = torch.tensor(train_df['label'].values)\n","                # val_labels = torch.tensor(val_df['label'].values)\n","\n","                train_data = list(zip(train, train_labels))\n","                dl_train = DataLoader(train_data, batch_size = training_config['batch_size'])\n","                # val_data = list(zip(val, val_labels))\n","                # dl_val = DataLoader(val_data, batch_size = training_config['batch_size'])\n","                dl_val = dl_test\n","\n","                torch.manual_seed(42)\n","                np.random.seed(42)\n","                mlp_config['num_features'] = len(feature_columns)\n","                model_MLP = MLP(mlp_config)\n","                model_MLP.apply(initialize_weights)\n","                # calculate class weights for training\n","                if torch.any(torch.isnan(training_config['class_weights'])):\n","                  u_labels, counts = torch.unique(train_labels, return_counts=True)\n","                  class_weights = 1/counts\n","                  training_config['class_weights'] = class_weights\n","\n","                # Training\n","                training_mlp = Training(model_MLP, training_config)\n","                training_mlp.train() #end of training loads the best model\n","                plot_training_summary(training_mlp)\n","                print('finished training')\n","\n","                training_mlp.evaluate_epoch(dl_val)\n","                # aggregate metrics\n","                uav_precision_m = training_mlp.report['uav']['precision']\n","                uav_recall_m = training_mlp.report['uav']['recall']\n","                uav_f1_m = training_mlp.report['uav']['f1-score']\n","                weighted_f1_m = training_mlp.report['weighted avg']['f1-score']\n","                # finally with test data\n","                training_mlp.evaluate_epoch(dl_test)\n","                test_report = training_mlp.report\n","\n","                # wrong_ind = np.where(predictions != test_labels)\n","                # print('wrong on:')\n","                # print(test_df['file'].iloc[wrong_ind])\n","                test_files = np.unique(test_df['file'])\n","                exp_record.loc[len(exp_record)] = [date_time, dur, test_files, split_config.copy(), features_config, model_name, model_config.copy(), copy.deepcopy(training_config),\n","                                  test_report, uav_precision_m, uav_recall_m, uav_f1_m, weighted_f1_m]\n","                # exp_record.to_csv(exp_record_path, index = False)"],"metadata":{"id":"QfM7V3qc6I8A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"bY8Bh2yqIQlY"}},{"cell_type":"code","source":["# training_mlp.report"],"metadata":{"id":"OEiT2GAWnGnH","executionInfo":{"status":"aborted","timestamp":1706336718364,"user_tz":-120,"elapsed":6,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["date_time_ = str(date_time).replace(\"/\",\"_\")\n","results_path = './Features Extraction Results/features_extraction_results_' + date_time_\n","\n","with open(results_path , 'wb') as f:\n","  pickle.dump(exp_record, f)"],"metadata":{"id":"MRJtp4JPMCVH","executionInfo":{"status":"ok","timestamp":1706337083572,"user_tz":-120,"elapsed":452,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# for rep in exp_record['Evaluation Report']: #mean std normalization\n","#   print(rep)"],"metadata":{"id":"PIiYWj7C0rE3","executionInfo":{"status":"aborted","timestamp":1706336718364,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for rep in exp_record['Evaluation Report']: #min_max normalization\n","#   print(rep)"],"metadata":{"id":"yh2A4z1JJvRR","executionInfo":{"status":"aborted","timestamp":1706336718364,"user_tz":-120,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for rep in exp_record['Evaluation Report']: #min_max normalization\n","  print(rep)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QY6hD_zcNMX","executionInfo":{"status":"ok","timestamp":1706337089054,"user_tz":-120,"elapsed":805,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"f6217c13-7bd6-4e6b-d2cf-1a10a106f0fb"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["{'airplane': {'precision': 0.9139784946236559, 'recall': 0.9826589595375722, 'f1-score': 0.9470752089136489, 'support': 173}, 'uav': {'precision': 0.8818565400843882, 'recall': 0.9047619047619048, 'f1-score': 0.8931623931623932, 'support': 231}, 'bird': {'precision': 0.8823529411764706, 'recall': 0.38461538461538464, 'f1-score': 0.5357142857142858, 'support': 39}, 'static-object': {'precision': 0.971976401179941, 'recall': 0.9762962962962963, 'f1-score': 0.9741315594974131, 'support': 675}, 'accuracy': 0.9418604651162791, 'macro avg': {'precision': 0.912541094266114, 'recall': 0.8120831363027895, 'f1-score': 0.8375208618219353, 'support': 1118}, 'weighted avg': {'precision': 0.9412548978816893, 'recall': 0.9418604651162791, 'f1-score': 0.9379214523847809, 'support': 1118}}\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"14M2RjCdLP7LuOlMIisnW1rAux96vU-N2","timestamp":1706336116222},{"file_id":"1r3QlF2dWJaA7LVwNG0NcHRBC7RAv-28R","timestamp":1705240362746},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"gpuType":"V100","authorship_tag":"ABX9TyPBta9KN18svBZHRPvWfi8z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
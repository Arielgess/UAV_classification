{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **Overview**\n","This notebook is a variation over Preprocessing for Mamba\n","It was used to produce test samples that are interpolated but not cleaned as part of the research to see what effects the results.\n","\n","The notebooks contains tools for preprocessing the Walaris dataset to Mamba compatible inputs. Each file is processed into samples of a known durtion. The samples are interpolated to also have the same length (within the same duration). The data is derived and collected into an 8-dimensional input. In addition, two inputs are calculated for each sample: the time interval vector and the scale:\n","1. The time interval vector for the iterpolated variation is trivial, but the implementation can take any real-tie interval.\n","2. The \"local Std\" is calculated for each parameter to serve as a normalizer during training, so an 8 dimensional vector of scales is saved with each sample.\n","\n","As explained in the report, we use samples of different but know duration to evaluate the perforance according to the duration. The samples we extract have a duration of 5, 10, 15, 20, 25, 30 and 60 second."]},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2hR_rZ4hcwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714313113562,"user_tz":-180,"elapsed":40342,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"24bd737a-808c-4413-b5c3-b0f0b533417a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP0cLHeSozdF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714313113562,"user_tz":-180,"elapsed":10,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"a48f10c5-02c5-43be-9f69-488d48f26784"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Segmentation for training"],"metadata":{"id":"Xqu6UXJ8qIcX"}},{"cell_type":"markdown","source":["with interpolation and derivation"],"metadata":{"id":"pbOZOr2jqNFF"}},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  # Derivation used for velocity and acceleration\n","  # Makes sur there are no inf or nan values\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","    '''This function segments a file according to the configurations.\n","  It performs cleaning and interpolation as needed and derives th data to obtain the velocity and acceleration and add them as inputs\n","  It also produces the time interval vector and the local Std (scale) vector, which is used for normaliation of the loss function.\n","  '''\n","  sample_duration = samples_config['sample_duration']\n","  skip_duration = samples_config['skip_duration']\n","  delta = samples_config['delta']\n","\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","\n","  # delta = 0.04\n","  interp_tt, interp_theta = interpolate_data(tt, theta, dt=delta, fixed = True)\n","  interp_tt, interp_phi = interpolate_data(tt, phi, dt=delta, fixed = True)\n","  interp_tt, interp_size_hor = interpolate_data(tt, size_hor, dt=delta, fixed = True)\n","  interp_tt, interp_size_ver = interpolate_data(tt, size_ver, dt=delta, fixed = True)\n","  dtt = np.diff(interp_tt)\n","  delta_tt = np.pad(dtt, (1, 0)) #this implementation works for time that is not interpolated - like during inference\n","\n","  # derive\n","  # expect possible trouble if np.interp doesn't work perfectly with the length of the outputs (in case of nans)\n","  interp_vel_theta, interp_vel_phi, interp_t_vel = derive(interp_theta, interp_phi, interp_tt)\n","  interp_acc_theta, interp_acc_phi, interp_t_acc = derive(interp_vel_theta, interp_vel_phi, interp_t_vel)\n","  interp_vel_theta = np.pad(interp_vel_theta, (1, 0))\n","  interp_vel_phi = np.pad(interp_vel_phi, (1, 0))\n","  interp_acc_theta = np.pad(interp_acc_theta, (2, 0))\n","  interp_acc_phi = np.pad(interp_acc_phi, (2, 0))\n","  complete_sample = np.stack([delta_tt, interp_theta, interp_phi, interp_vel_theta, interp_vel_phi, interp_acc_theta, interp_acc_phi, interp_size_hor, interp_size_ver]).T\n","\n","  #segmenting\n","  sample_length = int(np.round(sample_duration/delta) + 1)\n","  skip = int(np.round(skip_duration/delta) + 1)\n","  # skip_length = int(np.round(skip_duration/delta) + 1)\n","  #When we produce the test samples, we want no overlap between the samples\n","  #We pass 0 to signal that we want test samples and then the skip parameter will be the maximum between the sample length and the sent skip\n","  #The reason is that if we use sample length alone we could have too many samples, so this is how we can regulate\n","  # if for_test:\n","  #   skip = sample_length\n","  #   # skip = max(skip_length, sample_length)\n","  # else:\n","  #   skip = skip_length\n","  sub_samples = []\n","  sub_dt = []\n","  start_index = 0\n","  end_index = start_index + sample_length\n","\n","  while end_index <= len(interp_tt):\n","    #make sure initial sample has enough information\n","    start_tt_index = np.nonzero(tt>=(start_index-1)*delta)[0][0]\n","    end_tt_index = np.nonzero(tt>=(end_index-1)*delta)[0][0]\n","    if end_tt_index - start_tt_index < samples_config['min_samples']:\n","      start_index += skip\n","      end_index = start_index + sample_length\n","      continue\n","    #extract segment\n","    complete_tensor = torch.tensor(complete_sample[start_index:end_index-1, 1:], dtype = torch.float32)\n","    dt_tensor = torch.tensor(complete_sample[start_index:end_index-1, 0], dtype = torch.float32)\n","    sub_samples.append(complete_tensor)\n","    sub_dt.append(dt_tensor)\n","    start_index += skip\n","    end_index = start_index + sample_length\n","\n","  #scales\n","  scale_theta = torch.tensor(local_std(interp_theta, 10), dtype = torch.float32)\n","  scale_phi = torch.tensor(local_std(interp_phi, 10), dtype = torch.float32)\n","  scale_vel_theta = torch.tensor(local_std(interp_vel_phi, 10), dtype = torch.float32)\n","  scale_vel_phi = torch.tensor(local_std(interp_vel_phi, 10), dtype = torch.float32)\n","  scale_acc_theta = torch.tensor(local_std(interp_acc_theta, 10), dtype = torch.float32)\n","  scale_acc_phi = torch.tensor(local_std(interp_acc_phi, 10), dtype = torch.float32)\n","  scale_size_hor = torch.tensor(local_std(interp_size_hor, 10), dtype = torch.float32)\n","  scale_size_ver = torch.tensor(local_std(interp_size_ver, 10), dtype = torch.float32)\n","  scale = torch.stack([scale_theta, scale_phi, scale_vel_theta, scale_vel_phi, scale_acc_theta, scale_acc_phi, scale_size_hor, scale_size_ver]).T\n","\n","  return sub_samples, sub_dt, scale"],"metadata":{"id":"RL5aYuYSd0v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["labels_dict"],"metadata":{"id":"6KbmQvmQbP3U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714313113563,"user_tz":-180,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"fd8acd64-abea-4ffc-d6b1-c08cb4018971"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  #Extracts sample from an entire subfolder and returns it as a dict of subsamples by filename\n","  subfolder = samples_config['subfolder']\n","  samples_dict = {}\n","  total_samples = 0\n","  subfolder_path = os.path.join(folder, subfolder)\n","  files = os.listdir(subfolder_path)\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time(file_path, samples_config)\n","      # if len(sub_samples) == 0:\n","      #   # continue\n","      #   print(f'No samples found for {file}')\n","      samples_dict[file] = (sub_samples, sub_dt, scale)\n","      total_samples = total_samples + len(sub_samples)\n","  # print(f'A total of {total_samples} samples are taken from this folder')\n","  return samples_dict"],"metadata":{"id":"o_AFCYTkh3Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 15, 20, 25, 30, 60],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","for_test = [False, True]"],"metadata":{"id":"vND7C4RhoRC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flying_objects = ['airplane', 'uav', 'bird', 'static-object']\n","skips = [15, 15, 2, 40]"],"metadata":{"id":"F0HFhw5sFQXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Execute samples extraction for all subfolders and all durations according to the configuration and saves the results as pickles\n","sample_durations = samples_config['sample_durations']\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  for dur in sample_durations:\n","    print('Sample Duration = ', dur)\n","    samples_config['sample_duration'] = dur\n","    samples_config['min_samples'] = dur # this sets the minimal average rate of points to be 1 per sec for a valid sample\n","    for bo in for_test:\n","      samples_config['for_test'] = bo\n","      if bo:\n","        skip = 0\n","        # print('for test:')\n","      else:\n","        skip = samples_config['skip_duration']\n","        # print('for train:')\n","\n","      samples_dict = extract_samples(folder, samples_config)\n","      save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip) + '_interpolated'\n","      with open(save_path , 'wb') as f:\n","          pickle.dump(samples_dict, f)"],"metadata":{"id":"Suibu0DoFqWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714313468153,"user_tz":-180,"elapsed":353907,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"f4f9c93d-c923-442a-cc79-a5cc4f362751"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n"]}]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def split_by_scale(samples_config, split_config):\n","  #This function performs data splitting while trying to perserve the distribution of local std (according to azimuth)\n","  dur = 5 #smallest - to make sure we get all filenames\n","  skip = samples_config['skip_duration']\n","  save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip)\n","  with open(save_path , 'rb') as f:\n","    samples_dict = pickle.load(f)\n","\n","  subfolder_path = os.path.join(folder, samples_config['subfolder'])\n","  files = np.array(list(samples_dict.keys()))\n","  scales = []\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time(file_path, samples_config)\n","      scales.append(scale[0])\n","\n","  scales = np.array(scales)\n","  sorted_files = files[np.argsort(scales)]\n","  sorted_scales = scales[np.argsort(scales)]\n","  ratio_skip = int(1/split_config['test_split_ratio'])\n","  files_test = sorted_files[ratio_skip-1::ratio_skip]\n","  files_val = sorted_files[ratio_skip-2::ratio_skip]\n","  files_train = [file for file in files if (file not in files_test and file not in files_val)]\n","\n","  save_split_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + '_split0' + str(split_config['test_split_ratio'])\n","  with open(save_split_path , 'wb') as f:\n","      pickle.dump((files_train, files_val, files_test), f)\n","\n","  return files_train, files_val, files_test"],"metadata":{"id":"IQyC7rXCgQJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_dataset(samples_config, split_config):\n","  # This function uses split_by_scale to split the data into trainig/validation/test and saves them to memory\n","  # Each dataset is organized as a dictionary according to the samples lengths\n","  ts = split_config['test_split_ratio']\n","  skip = samples_config['skip_duration']\n","  files_train, files_val, files_test = split_by_scale(samples_config, split_config)\n","\n","  # print('collecting samples')\n","  all_train_data = {}\n","  all_test_data = {}\n","  all_val_data = {}\n","  train_sizes = []\n","  val_sizes = []\n","  test_sizes = []\n","\n","  for dur in samples_config['sample_durations']:\n","    save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip)\n","    with open(save_path , 'rb') as f:\n","      samples_dict = pickle.load(f)\n","    train_samples = []\n","    train_samples_filenames = []\n","    train_dt = []\n","    train_scales = []\n","\n","    for file in files_train:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      train_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      train_samples_filenames.extend(sub_file)\n","      train_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      train_scales.extend(sub_scale)\n","    train_sizes.append(len(train_samples))\n","    # print(len(train_samples))\n","    all_train_data[dur] = (train_samples, train_samples_filenames, train_dt, train_scales)\n","  for dur in samples_config['eval_sample_durations']:\n","    #take from zero overlap dict- no longer relevant\n","    save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(0)\n","    with open(save_path , 'rb') as f:\n","      samples_dict = pickle.load(f)\n","    val_samples = []\n","    val_samples_filenames = []\n","    val_dt = []\n","    val_scales = []\n","    test_samples = []\n","    test_samples_filenames = []\n","    test_dt = []\n","    test_scales = []\n","\n","    for file in files_val:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      val_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      val_samples_filenames.extend(sub_file)\n","      val_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      val_scales.extend(sub_scale)\n","    val_sizes.append(len(val_samples))\n","    # print(len(val_samples))\n","    all_val_data[dur] = (val_samples, val_samples_filenames, val_dt, val_scales)\n","\n","    for file in files_test:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      test_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      test_samples_filenames.extend(sub_file)\n","      test_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      test_scales.extend(sub_scale)\n","    test_sizes.append(len(test_samples))\n","    # print(len(test_samples))\n","    all_test_data[dur] = (test_samples, test_samples_filenames, test_dt, test_scales)\n","\n","  save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(ts) + '_val_test_interp_samples'\n","  with open(save_path , 'wb') as f:\n","    pickle.dump((all_train_data, all_val_data, all_test_data), f)\n","  # print('Done')\n","  return train_sizes, val_sizes, test_sizes"],"metadata":{"id":"jdQPqKaEdya_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 30, 60],\n","    'eval_sample_durations' : [5, 10, 15, 20, 25, 30],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","\n","split_config = {\n","    'random_state' : 24,\n","    'test_split_ratio' : 0.2,\n","  }\n","train_summary = pd.DataFrame(columns = samples_config['sample_durations'])\n","val_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","test_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  train_sizes, val_sizes, test_sizes = prepare_dataset(samples_config, split_config)\n","  train_summary.loc[len(train_summary)] = train_sizes\n","  val_summary.loc[len(val_summary)] = val_sizes\n","  test_summary.loc[len(test_summary)] = test_sizes\n","\n","train_summary.index = flying_objects\n","val_summary.index = flying_objects\n","test_summary.index = flying_objects\n","\n","print(train_summary)\n","print(val_summary)\n","print(test_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulWH2T2vEm9i","executionInfo":{"status":"ok","timestamp":1714313535125,"user_tz":-180,"elapsed":66974,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"04f2d185-b892-48f0-a713-dbef7a7dd995"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                5    10   30   60\n","airplane       218  189  102   51\n","uav            209  199  158  101\n","bird           267  220  109   42\n","static-object  317  307  246  106\n","                5    10  15  20  25  30\n","airplane        61   54  43  35  30  27\n","uav             77   75  72  67  65  62\n","bird            82   68  55  49  41  34\n","static-object  112  107  99  94  90  86\n","                5    10   15  20  25  30\n","airplane        63   57   46  38  33  28\n","uav             92   89   86  82  79  76\n","bird            58   43   31  23  19  15\n","static-object  112  109  106  99  91  86\n"]}]},{"cell_type":"code","source":["save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(split_config['test_split_ratio']) + '_val_test_interp_samples'\n","\n","with open(save_path , 'rb') as f:\n","  # train_samples, test_samples, train_samples_filenames, test_samples_filenames, train_dt, test_dt, train_scales, test_scales = pickle.load(f)\n","  (all_train_data, all_val_data, all_test_data) = pickle.load(f)"],"metadata":{"id":"pM8gEsgTucEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader"],"metadata":{"id":"EE9YwCJ9vA8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataloaders = []\n","for dur, (train_samples, train_samples_filenames, train_dt, train_scales) in all_train_data.items():\n","  dur_vec = [dur]*len(train_samples)\n","  test_data = list(zip(train_samples, train_dt, train_scales, dur_vec))\n","  if len(test_data) > 0:\n","    dataloaders.append(DataLoader(test_data, batch_size = 2))\n","    sample, dt, scale, duration = next(iter(dataloaders[-1]))\n","    print(sample.shape)\n","    print(dt.shape)\n","    print(scale.shape)\n","    print(duration.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F88msQEiusyD","executionInfo":{"status":"ok","timestamp":1714313749516,"user_tz":-180,"elapsed":4,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"b9d2e183-50bd-4079-b740-3ffdfa9e0c5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 125, 8])\n","torch.Size([2, 125])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 250, 8])\n","torch.Size([2, 250])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 750, 8])\n","torch.Size([2, 750])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 1500, 8])\n","torch.Size([2, 1500])\n","torch.Size([2, 8])\n","torch.Size([2])\n"]}]},{"cell_type":"code","source":["dataloaders = []\n","for dur, (train_samples, train_samples_filenames, train_dt, train_scales) in all_test_data.items():\n","  dur_vec = [dur]*len(train_samples)\n","  test_data = list(zip(train_samples, train_dt, train_scales, dur_vec))\n","  if len(test_data) > 0:\n","    dataloaders.append(DataLoader(test_data, batch_size = 2))\n","    sample, dt, scale, duration = next(iter(dataloaders[-1]))\n","    print(sample.shape)\n","    print(dt.shape)\n","    print(scale.shape)\n","    print(duration.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m08p_tlwV_O-","executionInfo":{"status":"ok","timestamp":1714313752939,"user_tz":-180,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"8adabcb9-28cd-4604-ecba-aaf9a9ceaf99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 125, 8])\n","torch.Size([2, 125])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 250, 8])\n","torch.Size([2, 250])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 375, 8])\n","torch.Size([2, 375])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 500, 8])\n","torch.Size([2, 500])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 625, 8])\n","torch.Size([2, 625])\n","torch.Size([2, 8])\n","torch.Size([2])\n","torch.Size([2, 750, 8])\n","torch.Size([2, 750])\n","torch.Size([2, 8])\n","torch.Size([2])\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1m9NUNXTMG6kwT61SqHS20MvVvKIznFNy","timestamp":1714312813228},{"file_id":"1JZ0CIdRE5RS4HYDXYraebL2hZ-H58ZYI","timestamp":1712125878750},{"file_id":"144vfLJLvIUtmQYb7zJuIPY8FHW2Y9Yeg","timestamp":1711218530284},{"file_id":"1qzltapyB28n96yMxozbXMKCuQNL6HgLv","timestamp":1709722343530},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"authorship_tag":"ABX9TyMhc0cKsadx51gEwknpk8ib"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
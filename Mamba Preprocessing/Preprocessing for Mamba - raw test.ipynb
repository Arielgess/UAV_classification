{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **Overview**\n","This notebook is a variation of \"processing for Mamba\".\n","It was used to produce test samples that are not cleaned or interploated in the research to see the effect on the results.\n","\n","The notebooks contains tools for preprocessing the Walaris dataset to Mamba compatible inputs. Each file is processed into samples of a known durtion. The samples are interpolated to also have the same length (within the same duration). The data is derived and collected into an 8-dimensional input. In addition, two inputs are calculated for each sample: the time interval vector and the scale:\n","1. The time interval vector for the iterpolated variation is trivial, but the implementation can take any real-tie interval.\n","2. The \"local Std\" is calculated for each parameter to serve as a normalizer during training, so an 8 dimensional vector of scales is saved with each sample.\n","\n","As explained in the report, we use samples of different but know duration to evaluate the perforance according to the duration. The samples we extract have a duration of 5, 10, 15, 20, 25, 30 and 60 second.\n"]},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2hR_rZ4hcwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714310093450,"user_tz":-180,"elapsed":36960,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"f2d23dfc-081c-41e5-96d1-347f26c71f2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP0cLHeSozdF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714310099059,"user_tz":-180,"elapsed":283,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"288d1421-81ad-4e3b-f9fb-924ad88b4ca4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Segmentation"],"metadata":{"id":"Xqu6UXJ8qIcX"}},{"cell_type":"markdown","source":["with derivation"],"metadata":{"id":"pbOZOr2jqNFF"}},{"cell_type":"code","source":["# Since we are not working with interpolation it is critical here to make sure we have single entries per timestamp ot the derivation would get inf values.\n","def get_unique_time_and_value(time_vec, value_vec):\n","  unique_time_vec = []\n","  unique_val_vec = []\n","  for i in range(len(time_vec)):\n","    if i == 0 or time_vec[i] != time_vec[i-1]:\n","      num_dup = 1\n","      unique_time_vec.append(time_vec[i])\n","      unique_val_vec.append(value_vec[i])\n","    else:\n","      num_dup = num_dup + 1\n","      unique_val_vec[-1] = ((num_dup-1)*unique_val_vec[-1] + value_vec[i])/num_dup\n","  return np.array(unique_time_vec), np.array(unique_val_vec)\n"],"metadata":{"id":"l-ULXjYvvJ1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  # Derivation used for velocity and acceleration\n","  # Protections for inf should not be relevant since it is assumes unique vectors are sent\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","    '''This function segments a file according to the configurations.\n","  This variation does not clean or interpolate the data, only derives it to obtain the velocity and acceleration and add them as inputs\n","  It also produces the time interval vector and the local Std (scale) vector, which is used for normaliation of the loss function.\n","  '''\n","  sample_duration = samples_config['sample_duration']\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  #return unique values (for later derivation)\n","  _, theta = get_unique_time_and_value(tt, theta)\n","  _, phi = get_unique_time_and_value(tt, phi)\n","  _, size_hor = get_unique_time_and_value(tt, size_hor)\n","  unique_tt, size_ver = get_unique_time_and_value(tt, size_ver)\n","  dtt = np.diff(unique_tt)\n","  delta_tt = np.pad(dtt, (1, 0))\n","  vel_theta, vel_phi, t_vel = derive(theta, phi, unique_tt)\n","  acc_theta, acc_phi, t_acc = derive(vel_theta, vel_phi, t_vel)\n","  vel_theta = np.pad(vel_theta, (1, 0))\n","  vel_phi = np.pad(vel_phi, (1, 0))\n","  acc_theta = np.pad(acc_theta, (2, 0))\n","  acc_phi = np.pad(acc_phi, (2, 0))\n","  # print(acc_theta.shape, acc_phi.shape, vel_phi.shape, vel_theta.shape, theta.shape, phi.shape, delta_tt.shape, size_hor.shape)\n","  complete_sample = np.stack([delta_tt, theta, phi, vel_theta, vel_phi, acc_theta, acc_phi, size_hor, size_ver]).T\n","\n","  #segmenting\n","  skip = samples_config['skip_duration']\n","  current_index = [0]\n","  end_index = np.nonzero(unique_tt>=unique_tt[0]+sample_duration)[0]\n","\n","  sub_samples = []\n","  sub_dt = []\n","  while end_index.size:\n","      if end_index[0] - current_index[0] > samples_config['min_samples']: ### a threshold for a minimum number of datapoints in a sample\n","        complete_tensor = torch.tensor(complete_sample[current_index[0]:end_index[0], 1:], dtype = torch.float32)\n","        dt_tensor = torch.tensor(complete_sample[current_index[0]:end_index[0], 0], dtype = torch.float32)\n","        sub_samples.append(complete_tensor)\n","        sub_dt.append(dt_tensor)\n","      end_index = np.nonzero(unique_tt>=unique_tt[current_index[0]]+skip+sample_duration)[0]\n","      current_index = np.nonzero(unique_tt>=unique_tt[current_index[0]]+skip)[0]\n","\n","\n","  #scales\n","  scale_theta = torch.tensor(local_std(theta, 10), dtype = torch.float32)\n","  scale_phi = torch.tensor(local_std(phi, 10), dtype = torch.float32)\n","  scale_vel_theta = torch.tensor(local_std(vel_phi, 10), dtype = torch.float32)\n","  scale_vel_phi = torch.tensor(local_std(vel_phi, 10), dtype = torch.float32)\n","  scale_acc_theta = torch.tensor(local_std(acc_theta, 10), dtype = torch.float32)\n","  scale_acc_phi = torch.tensor(local_std(acc_phi, 10), dtype = torch.float32)\n","  scale_size_hor = torch.tensor(local_std(size_hor, 10), dtype = torch.float32)\n","  scale_size_ver = torch.tensor(local_std(size_ver, 10), dtype = torch.float32)\n","  scale = torch.stack([scale_theta, scale_phi, scale_vel_theta, scale_vel_phi, scale_acc_theta, scale_acc_phi, scale_size_hor, scale_size_ver]).T\n","\n","  return sub_samples, sub_dt, scale"],"metadata":{"id":"8nYSYJX0tR-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["labels_dict"],"metadata":{"id":"6KbmQvmQbP3U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714310153774,"user_tz":-180,"elapsed":271,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"346ee561-6f71-4562-83fa-dfa0a992487f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  #Extracts sample from an entire subfolder and returns it as a dict of subsamples by filename\n","  subfolder = samples_config['subfolder']\n","  samples_dict = {}\n","  total_samples = 0\n","  subfolder_path = os.path.join(folder, subfolder)\n","  files = os.listdir(subfolder_path)\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time(file_path, samples_config)\n","      # if len(sub_samples) == 0:\n","      #   # continue\n","      #   print(f'No samples found for {file}')\n","      samples_dict[file] = (sub_samples, sub_dt, scale)\n","      total_samples = total_samples + len(sub_samples)\n","  # print(f'A total of {total_samples} samples are taken from this folder')\n","  return samples_dict"],"metadata":{"id":"o_AFCYTkh3Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 15, 20, 25, 30, 60],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","for_test = [False, True]"],"metadata":{"id":"vND7C4RhoRC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flying_objects = ['airplane', 'uav', 'bird', 'static-object']\n","skips = [15, 15, 2, 40]"],"metadata":{"id":"F0HFhw5sFQXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Execute samples extraction for all subfolders and all durations according to the configuration and saves the results as pickles\n","sample_durations = samples_config['sample_durations']\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  for dur in sample_durations:\n","    print('Sample Duration = ', dur)\n","    samples_config['sample_duration'] = dur\n","    samples_config['min_samples'] = dur # this sets the minimal average rate of points to be 1 per sec for a valid sample\n","    for bo in for_test:\n","      samples_config['for_test'] = bo\n","      if bo:\n","        skip = 0\n","        # print('for test:')\n","      else:\n","        skip = samples_config['skip_duration']\n","        # print('for train:')\n","\n","      samples_dict = extract_samples(folder, samples_config)\n","      save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip) + '_raw'\n","      with open(save_path , 'wb') as f:\n","          pickle.dump(samples_dict, f)"],"metadata":{"id":"Suibu0DoFqWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714310714410,"user_tz":-180,"elapsed":548999,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"2d62cc44-78bd-494f-c5ab-47953d359bb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n"]}]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def split_by_scale(samples_config, split_config):\n","  #This function performs data splitting while trying to perserve the distribution of local std (according to azimuth)\n","  dur = 5 #smallest - to make sure we get all filenames\n","  skip = samples_config['skip_duration']\n","  save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip)\n","  with open(save_path , 'rb') as f:\n","    samples_dict = pickle.load(f)\n","\n","  subfolder_path = os.path.join(folder, samples_config['subfolder'])\n","  files = np.array(list(samples_dict.keys()))\n","  scales = []\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time(file_path, samples_config)\n","      scales.append(scale[0])\n","\n","  scales = np.array(scales)\n","  sorted_files = files[np.argsort(scales)]\n","  sorted_scales = scales[np.argsort(scales)]\n","  ratio_skip = int(1/split_config['test_split_ratio'])\n","  files_test = sorted_files[ratio_skip-1::ratio_skip]\n","  files_val = sorted_files[ratio_skip-2::ratio_skip]\n","  files_train = [file for file in files if (file not in files_test and file not in files_val)]\n","\n","  save_split_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + '_split' + str(split_config['test_split_ratio'])\n","  with open(save_split_path , 'wb') as f:\n","      pickle.dump((files_train, files_val, files_test), f)\n","\n","  return files_train, files_val, files_test"],"metadata":{"id":"IQyC7rXCgQJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_test_dataset(samples_config, split_config):\n","  # This function uses split_by_scale to split the data into trainig/validation/test and saves them to memory\n","  # Each dataset is organized as a dictionary according to the samples lengths\n","  ts = split_config['test_split_ratio']\n","  skip = samples_config['skip_duration']\n","  files_train, files_val, files_test = split_by_scale(samples_config, split_config)\n","\n","  # print('collecting samples')\n","  all_train_data = {}\n","  all_test_data = {}\n","  all_val_data = {}\n","  train_sizes = []\n","  val_sizes = []\n","  test_sizes = []\n","\n","  for dur in samples_config['eval_sample_durations']:\n","    save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(0) + '_raw'\n","    with open(save_path , 'rb') as f:\n","      samples_dict = pickle.load(f)\n","    val_samples = []\n","    val_samples_filenames = []\n","    val_dt = []\n","    val_scales = []\n","    test_samples = []\n","    test_samples_filenames = []\n","    test_dt = []\n","    test_scales = []\n","\n","    for file in files_val:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      val_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      val_samples_filenames.extend(sub_file)\n","      val_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      val_scales.extend(sub_scale)\n","    val_sizes.append(len(val_samples))\n","    # print(len(val_samples))\n","    all_val_data[dur] = (val_samples, val_samples_filenames, val_dt, val_scales)\n","\n","    for file in files_test:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      test_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      test_samples_filenames.extend(sub_file)\n","      test_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      test_scales.extend(sub_scale)\n","    test_sizes.append(len(test_samples))\n","    # print(len(test_samples))\n","    all_test_data[dur] = (test_samples, test_samples_filenames, test_dt, test_scales)\n","\n","  save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(ts) + '_val_test_raw_samples'\n","  with open(save_path , 'wb') as f:\n","    pickle.dump((all_val_data, all_test_data), f)\n","  # print('Done')\n","  return val_sizes, test_sizes"],"metadata":{"id":"jdQPqKaEdya_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 30, 60],\n","    'eval_sample_durations' : [5, 10, 15, 20, 25, 30],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","\n","split_config = {\n","    'random_state' : 24,\n","    'test_split_ratio' : 0.2,\n","  }\n","# train_summary = pd.DataFrame(columns = samples_config['sample_durations'])\n","val_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","test_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  val_sizes, test_sizes = prepare_test_dataset(samples_config, split_config)\n","  # train_summary.loc[len(train_summary)] = train_sizes\n","  val_summary.loc[len(val_summary)] = val_sizes\n","  test_summary.loc[len(test_summary)] = test_sizes\n","\n","# train_summary.index = flying_objects\n","val_summary.index = flying_objects\n","test_summary.index = flying_objects\n","\n","# print(train_summary)\n","print(val_summary)\n","print(test_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulWH2T2vEm9i","executionInfo":{"status":"ok","timestamp":1714310828350,"user_tz":-180,"elapsed":32005,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"e99f2089-30f7-4c85-b334-ddf516f494b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                5    10   15  20  25  30\n","airplane        67   58   48  42  37  31\n","uav             87   85   82  77  75  72\n","bird            67   54   39  31  22  18\n","static-object  107  106  100  95  92  87\n","                5    10   15  20  25  30\n","airplane        68   64   52  43  39  33\n","uav             96   92   88  86  83  78\n","bird            89   75   67  60  52  45\n","static-object  110  105  101  95  90  85\n"]}]},{"cell_type":"code","source":["save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(split_config['test_split_ratio']) + '_val_test_raw_samples'\n","with open(save_path , 'rb') as f:\n","  (all_val_data, all_test_data) = pickle.load(f)"],"metadata":{"id":"pM8gEsgTucEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader"],"metadata":{"id":"EE9YwCJ9vA8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since there is no interpolation here, we expect to get samples of various sizes and so we must use a batch size of 1 in a dataloader"],"metadata":{"id":"Hnjwv77PyniR"}},{"cell_type":"code","source":["dataloaders = []\n","for dur, (test_samples, test_samples_filenames, test_dt, test_scales) in all_test_data.items():\n","  dur_vec = [dur]*len(test_samples)\n","  test_data = list(zip(test_samples, test_dt, test_scales, dur_vec))\n","  if len(test_data) > 0:\n","    dataloaders.append(DataLoader(test_data, batch_size = 1))\n","    sample, dt, scale, duration = next(iter(dataloaders[-1]))\n","    print(sample.shape)\n","    print(dt.shape)\n","    print(scale.shape)\n","    print(duration.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m08p_tlwV_O-","executionInfo":{"status":"ok","timestamp":1714310851261,"user_tz":-180,"elapsed":285,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"3152982a-88c6-48b8-95a3-a15c3c1f537f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 100, 8])\n","torch.Size([1, 100])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 222, 8])\n","torch.Size([1, 222])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 347, 8])\n","torch.Size([1, 347])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 472, 8])\n","torch.Size([1, 472])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 595, 8])\n","torch.Size([1, 595])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 720, 8])\n","torch.Size([1, 720])\n","torch.Size([1, 8])\n","torch.Size([1])\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1m9NUNXTMG6kwT61SqHS20MvVvKIznFNy","timestamp":1713953425415},{"file_id":"1JZ0CIdRE5RS4HYDXYraebL2hZ-H58ZYI","timestamp":1712125878750},{"file_id":"144vfLJLvIUtmQYb7zJuIPY8FHW2Y9Yeg","timestamp":1711218530284},{"file_id":"1qzltapyB28n96yMxozbXMKCuQNL6HgLv","timestamp":1709722343530},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"authorship_tag":"ABX9TyNo5uwv1XZGpUjCcY2rkRAL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"uj75cMRiwpym"},"source":["### **Overview**\n","This notebook is a variation of \"processing for Mamba\".\n","It was used to produce test samples that are cleaned but not interploated in the research to see the effect on the results.\n","\n","The notebooks contains tools for preprocessing the Walaris dataset to Mamba compatible inputs. Each file is processed into samples of a known durtion. The samples are interpolated to also have the same length (within the same duration). The data is derived and collected into an 8-dimensional input. In addition, two inputs are calculated for each sample: the time interval vector and the scale:\n","1. The time interval vector for the iterpolated variation is trivial, but the implementation can take any real-tie interval.\n","2. The \"local Std\" is calculated for each parameter to serve as a normalizer during training, so an 8 dimensional vector of scales is saved with each sample.\n","\n","As explained in the report, we use samples of different but know duration to evaluate the perforance according to the duration. The samples we extract have a duration of 5, 10, 15, 20, 25, 30 and 60 second.\n"]},{"cell_type":"markdown","metadata":{"id":"IFad3OUXiH7m"},"source":["### Imports and loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2hR_rZ4hcwY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717431895542,"user_tz":-180,"elapsed":31934,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"447ec581-a05b-423f-c012-2a08ce049efb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# imports\n","\n","import io\n","import os\n","import sys\n","from datetime import datetime\n","import pickle\n","import warnings\n","warnings.filterwarnings('ignore')\n","import json\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from scipy.fft import fft\n","import pandas as pd\n","import seaborn as sns\n","from scipy import interpolate\n","from scipy.interpolate import interp1d\n","from scipy.spatial.distance import cdist\n","from scipy.stats import pearsonr\n","\n","# Machine Learning\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import classification_report\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","# helper files\n","sys.path.append('/content/drive/MyDrive/Final Project UAV/')\n","from UAV_project_preprocessing_and_visualization_helper_functions_full import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EP0cLHeSozdF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717431895543,"user_tz":-180,"elapsed":5,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"15fbe7f8-2b5e-4933-d129-c9ccbe9bc7e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Final Project UAV\n"]}],"source":["cd /content/drive/MyDrive/Final Project UAV/"]},{"cell_type":"code","source":["folder = 'track_data'"],"metadata":{"id":"CMND-GMFfeaM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Segmentation for training"],"metadata":{"id":"Xqu6UXJ8qIcX"}},{"cell_type":"markdown","source":["with derivation"],"metadata":{"id":"pbOZOr2jqNFF"}},{"cell_type":"code","source":["# Since we are not working with interpolation it is critical here to make sure we have single entries per timestamp ot the derivation would get inf values.\n","def get_unique_time_and_value(time_vec, value_vec):\n","  unique_time_vec = []\n","  unique_val_vec = []\n","  for i in range(len(time_vec)):\n","    if i == 0 or time_vec[i] != time_vec[i-1]:\n","      num_dup = 1\n","      unique_time_vec.append(time_vec[i])\n","      unique_val_vec.append(value_vec[i])\n","    else:\n","      num_dup = num_dup + 1\n","      unique_val_vec[-1] = ((num_dup-1)*unique_val_vec[-1] + value_vec[i])/num_dup\n","  return np.array(unique_time_vec), np.array(unique_val_vec)\n"],"metadata":{"id":"l-ULXjYvvJ1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def derive(theta_data, phi_data, time):\n","  # Derivation used for velocity and acceleration\n","  # Protections for inf should not be relevant since it is assumes unique vectors are sent\n","  t_der = (time[:-1] + time[1:])/2\n","  der_theta = np.diff(theta_data)/np.diff(time)\n","  inf_ind1 = np.where(np.isinf(der_theta))\n","  nan_ind1 = np.where(np.isnan(der_theta))\n","  der_phi = np.diff(phi_data)/np.diff(time)\n","  inf_ind2 = np.where(np.isinf(der_phi))\n","  nan_ind2 = np.where(np.isnan(der_phi))\n","  inf_inds = np.union1d(inf_ind1, inf_ind2)\n","  nan_inds = np.union1d(nan_ind1, nan_ind2)\n","  inds = np.union1d(inf_inds, nan_inds)\n","  mask = np.ones(len(t_der), dtype=bool)\n","  mask[inds] = False\n","  return der_theta[mask], der_phi[mask], t_der[mask]"],"metadata":{"id":"ZiJd1Au-gMOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time_classic(file_path, samples_config):\n","  sample_duration = samples_config['sample_duration']\n","  skip_duration = samples_config['skip_duration']\n","  delta = samples_config['delta']\n","\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","\n","  # delta = 0.04\n","  interp_tt, interp_theta = interpolate_data(tt, theta, dt=delta, fixed = True)\n","  interp_tt, interp_phi = interpolate_data(tt, phi, dt=delta, fixed = True)\n","  interp_tt, interp_size_hor = interpolate_data(tt, size_hor, dt=delta, fixed = True)\n","  interp_tt, interp_size_ver = interpolate_data(tt, size_ver, dt=delta, fixed = True)\n","  dtt = np.diff(interp_tt)\n","  delta_tt = np.pad(dtt, (1, 0)) #this implementation works for time that is not interpolated - like during inference\n","\n","  # derive\n","  # expect possible trouble if np.interp doesn't work perfectly with the length of the outputs (in case of nans)\n","  interp_vel_theta, interp_vel_phi, interp_t_vel = derive(interp_theta, interp_phi, interp_tt)\n","  interp_acc_theta, interp_acc_phi, interp_t_acc = derive(interp_vel_theta, interp_vel_phi, interp_t_vel)\n","  interp_vel_theta = np.pad(interp_vel_theta, (1, 0))\n","  interp_vel_phi = np.pad(interp_vel_phi, (1, 0))\n","  interp_acc_theta = np.pad(interp_acc_theta, (2, 0))\n","  interp_acc_phi = np.pad(interp_acc_phi, (2, 0))\n","  complete_sample = np.stack([delta_tt, interp_theta, interp_phi, interp_vel_theta, interp_vel_phi, interp_acc_theta, interp_acc_phi, interp_size_hor, interp_size_ver]).T\n","\n","  #segmenting\n","  sample_length = int(np.round(sample_duration/delta) + 1)\n","  skip = int(np.round(skip_duration/delta) + 1)\n","  # skip_length = int(np.round(skip_duration/delta) + 1)\n","  #When we produce the test samples, we want no overlap between the samples\n","  #We pass 0 to signal that we want test samples and then the skip parameter will be the maximum between the sample length and the sent skip\n","  #The reason is that if we use sample length alone we could have too many samples, so this is how we can regulate\n","  # if for_test:\n","  #   skip = sample_length\n","  #   # skip = max(skip_length, sample_length)\n","  # else:\n","  #   skip = skip_length\n","  sub_samples = []\n","  sub_dt = []\n","  start_index = 0\n","  end_index = start_index + sample_length\n","\n","  while end_index <= len(interp_tt):\n","    # print(end_index)\n","    # print(len(interp_tt))\n","    #make sure initial sample has enough information\n","    start_tt_index = np.nonzero(tt>=(start_index-1)*delta)[0][0]\n","    end_tt_index = np.nonzero(tt>=(end_index-1)*delta)[0][0]\n","    if end_tt_index - start_tt_index < samples_config['min_samples']:\n","      start_index += skip\n","      end_index = start_index + sample_length\n","      continue\n","    #extract segment\n","    complete_tensor = torch.tensor(complete_sample[start_index:end_index-1, 1:], dtype = torch.float32)\n","    dt_tensor = torch.tensor(complete_sample[start_index:end_index-1, 0], dtype = torch.float32)\n","    sub_samples.append(complete_tensor)\n","    sub_dt.append(dt_tensor)\n","    start_index += skip\n","    end_index = start_index + sample_length\n","\n","  #scales\n","  scale_theta = torch.tensor(local_std(interp_theta, 10), dtype = torch.float32)\n","  scale_phi = torch.tensor(local_std(interp_phi, 10), dtype = torch.float32)\n","  scale_vel_theta = torch.tensor(local_std(interp_vel_phi, 10), dtype = torch.float32)\n","  scale_vel_phi = torch.tensor(local_std(interp_vel_phi, 10), dtype = torch.float32)\n","  scale_acc_theta = torch.tensor(local_std(interp_acc_theta, 10), dtype = torch.float32)\n","  scale_acc_phi = torch.tensor(local_std(interp_acc_phi, 10), dtype = torch.float32)\n","  scale_size_hor = torch.tensor(local_std(interp_size_hor, 10), dtype = torch.float32)\n","  scale_size_ver = torch.tensor(local_std(interp_size_ver, 10), dtype = torch.float32)\n","  scale = torch.stack([scale_theta, scale_phi, scale_vel_theta, scale_vel_phi, scale_acc_theta, scale_acc_phi, scale_size_hor, scale_size_ver]).T\n","\n","  return sub_samples, sub_dt, scale"],"metadata":{"id":"RL5aYuYSd0v9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_file_by_time(file_path, samples_config):\n","  '''This function segments a file according to the configurations.\n","  This variation does not clean or interpolate the data, only derives it to obtain the velocity and acceleration and add them as inputs\n","  It also produces the time interval vector and the local Std (scale) vector, which is used for normaliation of the loss function.\n","  '''\n","  sample_duration = samples_config['sample_duration']\n","  tt, xx, yy, zz, theta, phi, size_hor, size_ver, light_domain = raw_angles_data_from_json(file_path)\n","  theta, phi = convert_to_angles(xx, yy, zz)\n","  theta, phi = clean_2D_data_w_split(tt, theta, phi, factor = 3, window = 5, threshold = -999)\n","  #return unique values (for later derivation)\n","  _, theta = get_unique_time_and_value(tt, theta)\n","  _, phi = get_unique_time_and_value(tt, phi)\n","  _, size_hor = get_unique_time_and_value(tt, size_hor)\n","  unique_tt, size_ver = get_unique_time_and_value(tt, size_ver)\n","  dtt = np.diff(unique_tt)\n","  delta_tt = np.pad(dtt, (1, 0))\n","  vel_theta, vel_phi, t_vel = derive(theta, phi, unique_tt)\n","  acc_theta, acc_phi, t_acc = derive(vel_theta, vel_phi, t_vel)\n","  vel_theta = np.pad(vel_theta, (1, 0))\n","  vel_phi = np.pad(vel_phi, (1, 0))\n","  acc_theta = np.pad(acc_theta, (2, 0))\n","  acc_phi = np.pad(acc_phi, (2, 0))\n","  complete_sample = np.stack([delta_tt, theta, phi, vel_theta, vel_phi, acc_theta, acc_phi, size_hor, size_ver]).T\n","\n","  #segmenting\n","  skip = samples_config['skip_duration']\n","  current_index = [0]\n","  end_index = np.nonzero(unique_tt>=unique_tt[0]+sample_duration)[0]\n","\n","  sub_samples = []\n","  sub_dt = []\n","  while end_index.size:\n","      if end_index[0] - current_index[0] > samples_config['min_samples']: ### a threshold for a minimum number of datapoints in a sample\n","        complete_tensor = torch.tensor(complete_sample[current_index[0]:end_index[0], 1:], dtype = torch.float32)\n","        dt_tensor = torch.tensor(complete_sample[current_index[0]:end_index[0], 0], dtype = torch.float32)\n","        sub_samples.append(complete_tensor)\n","        sub_dt.append(dt_tensor)\n","      end_index = np.nonzero(unique_tt>=unique_tt[current_index[0]]+skip+sample_duration)[0]\n","      current_index = np.nonzero(unique_tt>=unique_tt[current_index[0]]+skip)[0]\n","\n","\n","  #scales\n","  scale_theta = torch.tensor(local_std(theta, 10), dtype = torch.float32)\n","  scale_phi = torch.tensor(local_std(phi, 10), dtype = torch.float32)\n","  scale_vel_theta = torch.tensor(local_std(vel_phi, 10), dtype = torch.float32)\n","  scale_vel_phi = torch.tensor(local_std(vel_phi, 10), dtype = torch.float32)\n","  scale_acc_theta = torch.tensor(local_std(acc_theta, 10), dtype = torch.float32)\n","  scale_acc_phi = torch.tensor(local_std(acc_phi, 10), dtype = torch.float32)\n","  scale_size_hor = torch.tensor(local_std(size_hor, 10), dtype = torch.float32)\n","  scale_size_ver = torch.tensor(local_std(size_ver, 10), dtype = torch.float32)\n","  scale = torch.stack([scale_theta, scale_phi, scale_vel_theta, scale_vel_phi, scale_acc_theta, scale_acc_phi, scale_size_hor, scale_size_ver]).T\n","\n","  return sub_samples, sub_dt, scale"],"metadata":{"id":"8nYSYJX0tR-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["subfolders = os.listdir(\"track_data/\")\n","subf_dict = {i:subfolders[i] for i in range(len(subfolders))}\n","labels_dict = {subfolders[i]:i for i in range(len(subfolders))}"],"metadata":{"id":"uNSdEfjxoIdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Extract samples"],"metadata":{"id":"NSK_zw837Dox"}},{"cell_type":"code","source":["labels_dict"],"metadata":{"id":"6KbmQvmQbP3U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717431895543,"user_tz":-180,"elapsed":3,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"e2f32ef4-2e74-474a-ef8a-e30ad7f4235a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'airplane': 0, 'uav': 1, 'bird': 2, 'static-object': 3}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["def extract_samples(folder, samples_config):\n","  #Extracts sample from an entire subfolder and returns it as a dict of subsamples by filename\n","  subfolder = samples_config['subfolder']\n","  samples_dict = {}\n","  total_samples = 0\n","  subfolder_path = os.path.join(folder, subfolder)\n","  files = os.listdir(subfolder_path)\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time(file_path, samples_config)\n","      # if len(sub_samples) == 0:\n","      #   # continue\n","      #   print(f'No samples found for {file}')\n","      samples_dict[file] = (sub_samples, sub_dt, scale)\n","      total_samples = total_samples + len(sub_samples)\n","  # print(f'A total of {total_samples} samples are taken from this folder')\n","  return samples_dict"],"metadata":{"id":"o_AFCYTkh3Dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 15, 20, 25, 30, 60],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","for_test = [False, True]"],"metadata":{"id":"vND7C4RhoRC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flying_objects = ['airplane', 'uav', 'bird', 'static-object']\n","skips = [15, 15, 2, 40]"],"metadata":{"id":"F0HFhw5sFQXt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Execute samples extraction for all subfolders and all durations according to the configuration and saves the results as pickles\n","\n","sample_durations = samples_config['sample_durations']\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  for dur in sample_durations:\n","    print('Sample Duration = ', dur)\n","    samples_config['sample_duration'] = dur\n","    samples_config['min_samples'] = dur # this sets the minimal average rate of points to be 1 per sec for a valid sample\n","    for bo in for_test:\n","      samples_config['for_test'] = bo\n","      if bo:\n","        skip = 0\n","        # print('for test:')\n","      else:\n","        skip = samples_config['skip_duration']\n","        # print('for train:')\n","\n","      samples_dict = extract_samples(folder, samples_config)\n","      save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip) + '_cleaned'\n","      with open(save_path , 'wb') as f:\n","          pickle.dump(samples_dict, f)"],"metadata":{"id":"Suibu0DoFqWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717432264723,"user_tz":-180,"elapsed":318889,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"6fb23be2-ae22-4e95-eb26-f630827c5350"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n","Sample Duration =  5\n","Sample Duration =  10\n","Sample Duration =  15\n","Sample Duration =  20\n","Sample Duration =  25\n","Sample Duration =  30\n","Sample Duration =  60\n"]}]},{"cell_type":"markdown","source":["### Prepare dataset"],"metadata":{"id":"QbUS6Eo373XT"}},{"cell_type":"code","source":["def split_by_scale(samples_config, split_config):\n","  #This function performs data splitting while trying to perserve the distribution of local std (according to azimuth)\n","  dur = 5 #smallest - to make sure we get all filenames\n","  skip = samples_config['skip_duration']\n","  save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(skip)\n","  with open(save_path , 'rb') as f:\n","    samples_dict = pickle.load(f)\n","\n","  subfolder_path = os.path.join(folder, samples_config['subfolder'])\n","  files = np.array(list(samples_dict.keys()))\n","  scales = []\n","  for file in files:\n","      file_path = os.path.join(subfolder_path, file)\n","      sub_samples, sub_dt, scale = segment_file_by_time_classic(file_path, samples_config)\n","      scales.append(scale[0])\n","\n","  scales = np.array(scales)\n","  sorted_files = files[np.argsort(scales)]\n","  sorted_scales = scales[np.argsort(scales)]\n","  ratio_skip = int(1/split_config['test_split_ratio'])\n","  files_test = sorted_files[ratio_skip-1::ratio_skip]\n","  files_val = sorted_files[ratio_skip-2::ratio_skip]\n","  files_train = [file for file in files if (file not in files_test and file not in files_val)]\n","\n","  save_split_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + '_split' + str(split_config['test_split_ratio'])\n","  with open(save_split_path , 'wb') as f:\n","      pickle.dump((files_train, files_val, files_test), f)\n","\n","  return files_train, files_val, files_test"],"metadata":{"id":"IQyC7rXCgQJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_test_dataset(samples_config, split_config):\n","  ts = split_config['test_split_ratio']\n","  skip = samples_config['skip_duration']\n","  files_train, files_val, files_test = split_by_scale(samples_config, split_config)\n","\n","  # print('collecting samples')\n","  all_train_data = {}\n","  all_test_data = {}\n","  all_val_data = {}\n","  train_sizes = []\n","  val_sizes = []\n","  test_sizes = []\n","\n","  for dur in samples_config['eval_sample_durations']:\n","    #take from zero overlap dict - no longer relevant\n","    save_path = './Samples/mamba_samples_'+ samples_config['subfolder'] + str(dur) + 'skip' + str(0) + '_cleaned'\n","    with open(save_path , 'rb') as f:\n","      samples_dict = pickle.load(f)\n","    val_samples = []\n","    val_samples_filenames = []\n","    val_dt = []\n","    val_scales = []\n","    test_samples = []\n","    test_samples_filenames = []\n","    test_dt = []\n","    test_scales = []\n","\n","    for file in files_val:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      val_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      val_samples_filenames.extend(sub_file)\n","      val_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      val_scales.extend(sub_scale)\n","    val_sizes.append(len(val_samples))\n","    # print(len(val_samples))\n","    all_val_data[dur] = (val_samples, val_samples_filenames, val_dt, val_scales)\n","\n","    for file in files_test:\n","      sub_samples, sub_dt, scale = samples_dict[file]\n","      test_samples.extend(sub_samples)\n","      sub_file = [file]*len(sub_samples)\n","      test_samples_filenames.extend(sub_file)\n","      test_dt.extend(sub_dt)\n","      sub_scale = [scale]*len(sub_samples)\n","      test_scales.extend(sub_scale)\n","    test_sizes.append(len(test_samples))\n","    # print(len(test_samples))\n","    all_test_data[dur] = (test_samples, test_samples_filenames, test_dt, test_scales)\n","\n","  save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(ts) + '_val_test_cleaned_samples'\n","  with open(save_path , 'wb') as f:\n","    pickle.dump((all_val_data, all_test_data), f)\n","  # print('Done')\n","  return val_sizes, test_sizes"],"metadata":{"id":"jdQPqKaEdya_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["samples_config = {\n","    'subfolder' : 'bird',\n","    'delta' : 0.04,\n","    'sample_durations' : [5, 10, 30, 60],\n","    'eval_sample_durations' : [5, 10, 15, 20, 25, 30],\n","    'sample_duration' : 10,\n","    'skip_duration' : 2,\n","    'min_samples' : 10,\n","    'for_test': False\n","}\n","\n","split_config = {\n","    'random_state' : 24,\n","    'test_split_ratio' : 0.2,\n","  }\n","# train_summary = pd.DataFrame(columns = samples_config['sample_durations'])\n","val_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","test_summary = pd.DataFrame(columns = samples_config['eval_sample_durations'])\n","\n","for f_object, skip in zip(flying_objects, skips):\n","  # summary_columns = ['object', 'train set size', 'validation set size', 'test set size']\n","  # summary = pd.DataFrame(columns = summary_columns)\n","  samples_config['subfolder'] = f_object\n","  samples_config['skip_duration'] = skip\n","\n","  val_sizes, test_sizes = prepare_test_dataset(samples_config, split_config)\n","  # train_summary.loc[len(train_summary)] = train_sizes\n","  val_summary.loc[len(val_summary)] = val_sizes\n","  test_summary.loc[len(test_summary)] = test_sizes\n","\n","# train_summary.index = flying_objects\n","val_summary.index = flying_objects\n","test_summary.index = flying_objects\n","\n","# print(train_summary)\n","print(val_summary)\n","print(test_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulWH2T2vEm9i","executionInfo":{"status":"ok","timestamp":1717432295216,"user_tz":-180,"elapsed":30490,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"2e446c0e-5dd6-4dbe-fe1b-262586e68e1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                5    10  15  20  25  30\n","airplane        67   59  48  42  37  31\n","uav             81   79  76  71  69  66\n","bird            67   54  39  31  22  18\n","static-object  106  105  99  94  89  87\n","                5    10  15  20  25  30\n","airplane        62   57  45  37  33  27\n","uav             89   85  81  79  76  71\n","bird            89   75  67  60  52  45\n","static-object  109  105  98  92  87  82\n"]}]},{"cell_type":"code","source":["save_path = './Samples/mamba_samples_' + samples_config['subfolder'] + '_skip' + str(skip) +'_split' + str(split_config['test_split_ratio']) + '_val_test_cleaned_samples'\n","with open(save_path , 'rb') as f:\n","  (all_val_data, all_test_data) = pickle.load(f)"],"metadata":{"id":"pM8gEsgTucEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader"],"metadata":{"id":"EE9YwCJ9vA8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since there is no interpolation here, we expect to get samples of various sizes and so we must use a batch size of 1 in a dataloader"],"metadata":{"id":"MTzGQUjG0V88"}},{"cell_type":"code","source":["dataloaders = []\n","for dur, (test_samples, test_samples_filenames, test_dt, test_scales) in all_test_data.items():\n","  dur_vec = [dur]*len(test_samples)\n","  test_data = list(zip(test_samples, test_dt, test_scales, dur_vec))\n","  if len(test_data) > 0:\n","    dataloaders.append(DataLoader(test_data, batch_size = 1))\n","    it_data = iter(dataloaders[-1])\n","    sample, dt, scale, duration = next(it_data)\n","    print(sample.shape)\n","    print(dt.shape)\n","    print(scale.shape)\n","    print(duration.shape)\n","    sample, dt, scale, duration = next(it_data)\n","    print(sample.shape)\n","    print(dt.shape)\n","    print(scale.shape)\n","    print(duration.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m08p_tlwV_O-","executionInfo":{"status":"ok","timestamp":1717434010956,"user_tz":-180,"elapsed":256,"user":{"displayName":"אילה רענן","userId":"13457086393139042919"}},"outputId":"4f004fd0-8749-4106-ceae-cc5044e2caef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 125, 8])\n","torch.Size([1, 125])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 113, 8])\n","torch.Size([1, 113])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 250, 8])\n","torch.Size([1, 250])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 237, 8])\n","torch.Size([1, 237])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 375, 8])\n","torch.Size([1, 375])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 361, 8])\n","torch.Size([1, 361])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 500, 8])\n","torch.Size([1, 500])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 485, 8])\n","torch.Size([1, 485])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 625, 8])\n","torch.Size([1, 625])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 545, 8])\n","torch.Size([1, 545])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 750, 8])\n","torch.Size([1, 750])\n","torch.Size([1, 8])\n","torch.Size([1])\n","torch.Size([1, 665, 8])\n","torch.Size([1, 665])\n","torch.Size([1, 8])\n","torch.Size([1])\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"19T98K9PuklrSEYImO4GtswewJJMHlev6","timestamp":1717425041499},{"file_id":"1m9NUNXTMG6kwT61SqHS20MvVvKIznFNy","timestamp":1713953425415},{"file_id":"1JZ0CIdRE5RS4HYDXYraebL2hZ-H58ZYI","timestamp":1712125878750},{"file_id":"144vfLJLvIUtmQYb7zJuIPY8FHW2Y9Yeg","timestamp":1711218530284},{"file_id":"1qzltapyB28n96yMxozbXMKCuQNL6HgLv","timestamp":1709722343530},{"file_id":"1_etuvholW7g3X9SlbPZDu8d1cWIRUl-H","timestamp":1704450125309},{"file_id":"1motfF6rUUOGXhgrlE2CqgTQgjET8nvBQ","timestamp":1704356485850},{"file_id":"1BEZz0DElbumDJld8D-nA0DTSHSCKoxge","timestamp":1703350819753},{"file_id":"1OS076JBuQIYw2q3GOhVDLf3iQTMlHm0u","timestamp":1702562147127},{"file_id":"1A4ATexl9-Yz1pi-J4qIaeBIAdxAOloq8","timestamp":1702135302257},{"file_id":"1AqHpw3FlTGarVQSH1zfD5Kw8hUsgXyeN","timestamp":1702111543738},{"file_id":"1KZW2Q_CH5wo9_Uv0M5pfVKwPtVhMMhDH","timestamp":1701809788329}],"authorship_tag":"ABX9TyNWecyWaPaK7YHmQIRIr5/s"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}